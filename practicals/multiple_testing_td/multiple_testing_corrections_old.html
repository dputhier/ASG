<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> 

<html>

    <script language="JavaScript" type="text/javascript">
      <!--
	  function sizeTbl2(h,i) {
	  var tbl = document.getElementById(i);
	  tbl.style.display = h;
	  }
	  // -->
    </script> 

  <head>
    <meta content="text/html; charset=utf-8"
	  http-equiv="content-type">
    <title>Statistics for Bioinformatics - Practicals - [TITLE]</title>
    <link rel="stylesheet" type="text/css" href="../../html/course.css" />
    <script type="text/javascript" src="../syntaxhighlight/shCore.js"></script>
    <script type="text/javascript" src="../syntaxhighlight/shBrushBash.js"></script>
    <script type="text/javascript" src="../syntaxhighlight/shBrushR.js"></script>
    <link type="text/css" rel="stylesheet" href="../syntaxhighlight/shCore.css"/>
    <link type="text/css" rel="stylesheet" href="../syntaxhighlight/shThemeDefault.css"/>
    <script type="text/javascript">
      SyntaxHighlighter.config.clipboardSwf = '../../style/syntaxhighlight/clipboard.swf';
      SyntaxHighlighter.all();
    </script>
  </head>

  <body>
    <i>AMU :: M2 BBSG :: ASG1 :: 2012/2013 :: Denis Puthier & Jacques van Helden</i>

    <h1 class='title'>Statistics for Bioinformatics - Practicals - Multiple testing corrections</h1>


    <hr>
    <a name='contents'></a>
    <h2>Contents</h2>
    <ol>
      <li><a href='#intro'>Introduction</a></li>
      <li><a href='#random_datasets'>Generating random control sets</a></li>
      <li><a href='#p_value'>Density distributions of p-values</a></li>
      <li><a href='#pi0'>Estimating the proportions of truly null and truly alternative hypotheses</a></li>
      <li><a href='#multitesting_why'>Why do we need to correct for multiple testing ?</a></li>
      <li><a href='#e_value'>Expected number of false positives (e-value)</a></li>
      <li><a href='#fwer'>Family-Wise Error Rate</a></li>
      <li><a href='#q_value'>Controlling the False Discovery Rate (FDR) with the q-value</a></li>
      <li><a href='#biblio'>Bibliographic references</a></li>
    </ol>

    <hr><a name='intro'></a>
    <h2>Introduction</h2>

    <p>
      Most analyses in current bioinformatics consist in performing
      thousands, or even millions of statistical tests in
      parallel. This applies to the anlaysis of microarrays
      (e.g. differentially expressed genes), motif detection
      (e.g. discovering motifs in promoters of co-expressed genes),
      sequence similarity searches (comparing a query sequence against
      the millions of peptidic sequences currently available in
      Uniprot), etc. </p>

    <p>In a previous practical
      (<a href="../affy_selection_td/index.html">Selecting
      differentially expressed genes</a>), we applied a Welch test to
      select differentially expressed genes from a microarray series
      containing 190 ALL samples. By doing this, we successively
      tested for more than 22,000 probesets the equality of the mean
      expression values, between two classes of ALL. For each gene, we
      computed a <b>nominal p-value</b>, which indicates the
      probability to obtain by chance a difference at least as large
      as the one observed in the data. This p-value can be interpreted
      as an estimate of the <b>False positive risk</b> (<b>FPR</b>):
      the probability of considering an observation as significant
      whereas it is not. However, we did not take into account an
      important factor: since the same test was successively applied
      to 22,283 probeset, the risk of false positives was repeated for
      each probeset. This situation is classically denoted
      as <b>multiple testing</b>. For example, if we accept an
      individual risk of 1%, we expect to observe 1% * 22000=220 false
      positives when the same risk is taken for each probe of the
      microarray series.
    </p>
    <p>
      We will thus need to be more restrictive if we want to control
      the false positives. Several methods have been proposed to
      control the risk of false positives in situations of multiple
      testing. In this practical, we will investigate the practical
      consequences of multiple testing and explore some of the
      proposed solutions.
    </p>


    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='random_datasets'></a>
    <h2>Generating random control sets</h2>

    <p>In order to get an intuition of the problems arising from
      multiple testing, we will generate three datasets where no
      difference is expected to be found between the mean expression
      values of two groups of samples. 
    </p>

    <div class='exo'>
      <h3>Exercise</h3>
      <ol>
	<li>Reload the normalized expression matrix from DenBoer, as
	  described in the
	  practical "<a href="../affy_selection_td/index.html">Selecting
	    differentially expressed genes</a>"</li>
	<li>Generate an expression matrix of the same size, in a data
	  frame named "rnorm.matrix", and fill if with random values
	  sampled in a normal distribution of mean <i>m=0</i> and
	  standard deviation <i>sd=1</i>.</li>
	<li>Create a second data frame of the same size, name it
	  "denboer.permuted.values", and fill it with the actual values
	  of the DenBoer expression matrix, sampled in random order.
	  <p class='tip'>Random sapling can be done with the R
	    function <tt>sample()</tt>. Read the help page of this
	    function to know how to use it in this context.</p>
	</li>
	<li>
	  Create a vector name "denboer.permuted.subtypes" and fill it
	  with a random sampling of the cancer subtypes
	  (these values can be found in <tt>pheno$Sample_title</tt>).
	</li>
      </ol>
    </div>

    <a href="javascript:sizeTbl2('block','random_data')">View solution</a>| 
    <a href="javascript:sizeTbl2('none',random_data)">Hide solution</a>
    <div id="random_data"  style="overflow:hidden;display:none">
      <h4>Solution</h4>


<!--
## Load the DenBoer data set (old URLs)
url.base <- "http://www.bigre.ulb.ac.be/courses/statistics_bioinformatics/data/gene_expression/denboer_2009/"
expr.matrix <-  read.table(file.path(url.base, "GSE13425_Norm_Whole.txt"),sep="\t", head=T, row=1)
pheno <- read.table(file.path(url.base, 'phenoData_GSE13425.tab'), sep='\t', head=TRUE, row=1)
-->      

<pre class="brush:r">
## get the URL base, from which data will be loaded
url.base <- "http://pedagogix-tagc.univ-mrs.fr/courses/ASG1/data/marrays/"

## Load a library
if (!require("qvalue")) {
  source("http://bioconductor.org/biocLite.R")
  biocLite("qvalue")
}
library(qvalue)


## Load expression values
expr.matrix <-  read.table(file.path(url.base, "GSE13425_Norm_Whole.txt"),sep="\t", head=T, row=1)

## Load phenotypic data
pheno <- read.table(file.path(url.base, 'phenoData_GSE13425.tab'), sep='\t', head=TRUE, row=1)


## Read the help of the rnorm() R function
?rnorm

## Generate a matrix with random normal values
rnorm.matrix <- data.frame(matrix(rnorm(n=ncol(expr.matrix)*nrow(expr.matrix), mean=0, sd=1), ncol=ncol(expr.matrix), nrow=nrow(expr.matrix)))
colnames(rnorm.matrix) <- colnames(expr.matrix) ## Name the columns with same sample IDs as DenBoer, to facilitate the susbequent tests
rownames(rnorm.matrix) <- paste("rnorm", 1:nrow(rnorm.matrix), sep=".") ## Name the rows of the random number matrix
hist(as.matrix(rnorm.matrix), breaks=100) ## Check the distribution of the random normal sampling

## Read the help of the sample() R function
?sample
## Question: did you understand the role of the "replace" option ? If not, re-read the help.

## Generate a matrix with expression values resampled from DenBoer
denboer.values <- as.vector(as.matrix(expr.matrix))
denboer.perm.matrix <- data.frame(matrix(sample(x=denboer.values, size=length(denboer.values), replace=FALSE), ncol=ncol(expr.matrix), nrow=nrow(expr.matrix)))
colnames(denboer.perm.matrix) <- colnames(expr.matrix) ## Name the columns with same sample IDs as DenBoer, to facilitate the susbequent tests
rownames(denboer.perm.matrix) <- paste("perm.matrix", 1:nrow(rnorm.matrix), sep=".") ## Name the rows of the random number matrix
head(denboer.perm.matrix)
hist(as.matrix(denboer.perm.matrix), breaks=100) ## Check the distribution of the random normal sampling

## Generate a random vector of cancer subtypes by resampling the sample types of the phenodata
samples.to.keep <- pheno$Sample.title == "hyperdiploid" |  pheno$Sample.title == "TEL-AML1" ## Select samples of two defined types
cancer.type <- as.vector(pheno[samples.to.keep, "Sample.title"])  ## Define a vector with the sample types for the two selected cancer subtype
cancer.type.permuted <- sample(cancer.type) ## Permute the vector of sample types

## Compare original and permuted cancer type labels
table(cancer.type) ## Count the number of samples for each one of the selected cancer types
table(cancer.type.permuted) ## Count the number of samples for each one of the permuted cancer types
table(cancer.type, cancer.type.permuted) ## Compare the original and permuted cancer types
      </pre>

      <p>The <tt>table()</tt> command can be used either with a
	vector, to count the number of instance for each distinct
	value, or to generate a contingency table. The last command
	allowed us to compare the original and resampled vectors of
	sample labels (cancer types). In principle, the contingency
	table should be balanced: each of the original groups
	(hyperdiploid or TEL-AML1) has been split randomly among the
	two groups of the permuted vector. We obtained the following
	result.
<pre>
              cancer.type.permuted
cancer.type    hyperdiploid TEL-AML1
  hyperdiploid           23       21
  TEL-AML1               21       22
</pre>


      <p>Note that the results can vary from trial to trial. When
	trying a few more times, we obtain the following results,
	where the hyperdiploid and TEL-AML1 wwere partly unbalanced in
	the two random groups.</p>

      <pre class='brush:r'>
## Another permutation trial
cancer.type.permuted <- sample(cancer.type)
table(cancer.type, cancer.type.permuted)
</pre><pre>
              cancer.type.permuted
cancer.type    hyperdiploid TEL-AML1
  hyperdiploid           18       26
  TEL-AML1               26       17
</pre>

      <pre class='brush:r'>
## Yet another permutation trial
cancer.type.permuted <- sample(cancer.type)
table(cancer.type, cancer.type.permuted)
</pre><pre>
              cancer.type.permuted
cancer.type    hyperdiploid TEL-AML1
  hyperdiploid           24       20
  TEL-AML1               20       23
</pre>

      <p>Although sometimes unbalanced, the permutation will always be
	very far from the true repartition of the samples, which gives
	the following table.</p>

<pre class="brush:r">
## trivial comparison between the cancer.types and themselves, 
## for the sake of comparison with the confusion table of the 
##random permutataion
table(cancer.type,cancer.type)
</pre><pre>
              cancer.type
cancer.type    hyperdiploid TEL-AML1
  hyperdiploid           44        0
  TEL-AML1                0       43
</pre>
    </div>



    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='p_value'></a>
    <h2>Density distribution of the p-values</h2>


    <p>
      In the context of a Welch test, the p-value indicates the
      probability to obtain by chance a <i>t</i> statistics greater
      than to the one measured from the samples. Under the null
      hypothesis (i.e. if the data were sampled from populations with
      equal means) a p-value of 1% is expected to be found at random
      once in 100 tests, a p-value of 5% once in 20 tests, etc. The
      goal of this exercise is to test if the p-values returned by our
      multiple Welch test correspond to this expectation.
    </p>


    <div class='exo'>
      <h3>Exercise</h3>

      <p>For each of the 3 control sets prepared in the previous
	  section, and for the actual data set from DenBoer 2009,
	  apply the following procedure, and compare the results.
      <ol>
	<li>Run the Welch test on each probeset, using our custom
	  function <tt>t.test.multi()</tt> (as we did the practical
	  on <a href="../affy_selection_td/index.html">Selecting
	  differentially expressed genes</a>), to compare the two
	  subtypes "hyperdiploid" and "TEL-AML1".
	<li>Draw a plot comparing mean expression of "hyperdiploid"
	  (abcsissa) and "TEL-AML1" (ordinate), and highlight the
	  probesets declared significant with a p-value threshold of
	  1%. Count the number of significant genes and compare it to
	  the random expectation. </li>
	<li>Draw a histogram of the p-value density, by chunks of 5%,
	  and draw a line representing the theoretical
	  expectation for this distribution.</li>
      </ol>
    </div>

    <a href="javascript:sizeTbl2('block','sol_p_value')">View solution</a>| 
    <a href="javascript:sizeTbl2('none','sol_p_value')">Hide solution</a>
    <div id='sol_p_value' class='solution' style="overflow:hidden;display:none">
      <h3>Solutions</h3>
      

      <pre class='brush:r'>
## Load some libraries
source('http://www.bigre.ulb.ac.be/courses/statistics_bioinformatics/R-files/config.R')
source(file.path(dir.util, "util_student_test_multi.R"))

## Define the constants for drawing the histograms of p-value densities
pval.breaks <- 20 ## Number of class intervals for the p-value histograms
n.probesets <- nrow(expr.matrix) ## Number of probesets (rows in the expression matrix)
threshold <- 0.01 ## Significance threshold, that will be applied successively to p-value, e-value, FWER and q-value

## Open a graphical window for the 4 plots
x11(width=12, height=10)
par(mfrow=c(2,2))

## Run Welch test on normal-distributed random numbers
rnorm.welch <- t.test.multi(rnorm.matrix[,samples.to.keep], cancer.type, volcano.plot=FALSE)
print(sum(rnorm.welch$P.value <= threshold))## count the number of significant genes at a given FPR
hist(rnorm.welch$P.value, breaks=pval.breaks, main="Random normal values") ## Plot a histogram of p-value densities
abline(h=n.probesets / pval.breaks, col="darkgreen", lwd=2)

## Run Welch test on permuted matrix values
perm.matrix.welch <- t.test.multi(denboer.perm.matrix[,samples.to.keep], cancer.type, volcano.plot=FALSE)
print(sum(perm.matrix.welch$P.value <= threshold))## count the number of significant genes at a given FPR
hist(perm.matrix.welch$P.value, breaks=pval.breaks, main="Matrix-wise permuted expression values") ## Plot a histogram of p-value densities
abline(h=n.probesets / pval.breaks, col="darkgreen", lwd=2)

## Run Welch test on permuted samples
perm.labels.welch <- t.test.multi(expr.matrix[,samples.to.keep], cancer.type.permuted, volcano.plot=FALSE)
print(sum(perm.labels.welch$P.value <= threshold))## count the number of significant genes at a given FPR
hist(perm.labels.welch$P.value, breaks=pval.breaks, main="Permuted sample labels") ## Plot a histogram of p-value densities
abline(h=n.probesets / pval.breaks, col="darkgreen", lwd=2)

## Run Welch test on the dataset from Den Boer (2009)
denboer.welch <- t.test.multi(expr.matrix[,samples.to.keep], cancer.type, volcano.plot=FALSE) 
print(sum(denboer.welch$P.value <= threshold))## count the number of significant genes at a given FPR
hist(denboer.welch$P.value, breaks=pval.breaks, main="DenBoer, 2009") ## Plot a histogram of p-value densities
abline(h=n.probesets / pval.breaks, col="darkgreen", lwd=2) ## Draw the expected density under null hypothesis

## Store the figure in a file
setwd(dir.figures); export.plot(file.prefix='denboer_and_controls_pval_density_hist', export.formats=c("pdf", "png"), width=12,height=10)
      </pre>

      <h3>Interpretation of the results</h3>
      
      <p>We ran a Welch test on four datasets: 
	<ol>
	<li>Negative control 1: <b>Normally distributed random values</b>. This
	  control fits the working hypotheses for the Welch test: the
	  two populations (sample types) follow a normal distribution,
	  and have the same mean. </li>
	<li>Negative control 2: <b>permuted expression
	  values</b>. Random sampling, without replacement, of the
	  original expression values. Since these values do not
	  follow a normal distribution, we do not meet the working
	  hypotheses for a Welch test. However, Student and derived
	  tests (including Welch) are robust to non-normality of the
	  data, as soon as the number of elements (samples) is
	  sufficient.
	</li>
	<li>Negative control 3: normalized expression values from
	  DenBoer (2009), with <b>permuted sample labels</b> (cancer
	  types).</li>
	<li>Real test: <b>normalized  expression profiles</b> from
	<a href='#denboer_2009'>DenBoer et al., 2009</a>.</li>
	</ol>
      </p>
      <p>For each dataset, the multiple Welch test was applied to each
	of the 22,283 rows of the matrix, and returned 22,283
	p-values. The density plots indicate the repartition of
	p-values within 20 chunks of 5%. Under the null hypothesis
	(i.e. if not a single probe was differentially expressed), we
	would expect to obtain:
	<ul>
	  <li>a p-value &le; 5% in 5% of the tests,</li>
	  <li>a p-value comprised between 5% and 10% in 5% of the tests,</li>
	  <li>...</li>
	  <li>a p-value comprised between 95% and 100% in 5% of the tests,</li>
	</ul>
      <p>The density histograms are show below. Note that your result
	should be slightly different, since the three first histograms
	were generated from randomized datasets.
	  <p><a href='images/denboer_and_controls_pval_density_hist.jpg'>
	      <img width='600' bgcolor='white' src='images/denboer_and_controls_pval_density_hist.jpg'>
	  </a></p>
	</p>

	<p>The first (random normal values) and second (permuted
	  expression values) negative controls perform quite well: the
	  histograms are flat, and fit the random expectation (5% in
	  each class, marked with the horizontal green line). </p>
	<p>The third negative control is less obvious: low p-values
	  (&lt; 20%) are more frequent than expected by chance, and
	  high p-values are depleted. in some cases, the random
	  sampling of cancer types will proboke the opposite effect:
	  depletion of the low p-values and enrichment in high
	  p-values. The label sampling is a stringent test for
	  negative controls, and should be interpreted with
	  caution.</p>
	<p>The fourth density histogram shows a striking
	  over-representation of the low p-values (&lt; 5%). This
	  unbalance is similar to the one reported
	  by <a href='#storey_tibshirani_2003'>Storey and
	  Tibshirani</a> in Figure 1 of their 2003 article. The
	  over-representation of low p-values is likely to reflect the
	  presence of a large number of differentially expressed
	  genes. </p>
      </p>

    </div>


    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='pi0'></a>
    <h2>Estimating the proportions of truly null and truly alternative hypotheses</h2>

    <p>As discussed in the solution of the previous section, the
      density histogram of p-values suggests that the DenBoer dataset
      contains a mixture of probesets corresponding either of two categories:
      <ol>
	<li>Genes whose mean expression value does not differ between
	  the two considered cancer types (hyperdiploid and
	  TEL_AML1). The corresponding probesets are called "truly
	  null", because they fit the null hypothesis:
	  <ul><i>H<sub>0</sub>: m<sub>hyper</sub> = m<sub>TEL_ALL1</sub></i></ul>
	  The number of truly null probesets will be
	  denoted <i>m<sub>0</sub></i>.
	</li>
	<li>Genes expressed differentially between the two considered
	  cancer types. The corresponding probeset are called "truly
	  alternative" because they fit the alternative hypothesis.
	  <ul><i>H<sub>1</sub>: m<sub>hyper</sub> &ne; m<sub>TEL_ALL1</sub></i></ul>
	  The number of truly alternative probesets will be
	  denoted <i>m<sub>1</sub></i>.
      </ol>
      
    <p><a href='storey_tibshirani_2003'>Storey and
	Tibshirani (2003)</a> proposed an elegant way to estimate the
      number of probesets belonging to these two categories.
    </p>
    <p>The next exercise will lead you, step by step, to discover how
      Storey and Tibshirani estimate the numbers of truly null
      (m<sub>0</sub>) and alternative (m<sub>1</sub>) hypothesis.
    </p>

    <div class='exo'>
      <h3>Exercise</h3>
      <ol>
	<li>In the result table of the Welch test (DenBoer dataset),
	  count the number of probesets having a p-value &ge; 0.6 (let
	  us call the threshold p-value <i>&lambda;</i> ("lambda"),
	  and the number of probeset above the &lambda; p-value
	   <i>n<sub>&lambda;</sub></i>).</li>
	<li>We can reasonably assume that the truly alternate
	  probesets will tend to be concentrated in the low range of
	  p-values on the density histogram. Consequently, the we can
	  suppose that the area of the histogram covering p-values
	  from 0.6 to 1 is principally made of truly null
	  probesets. On the basis of the
	  number <i>n<sub>&lambda;</sub></i> (with <i>&lambda;=60</i>),
	  try to estimate
	    <ul>
	      <li>the total number <i>m<sub>0</sub></i> of truly null
		probesets.</li>
	      <li>the total number <i>m<sub>1</sub></i> of truly alternative
		probesets.</li>
	      <li>The proportion <i>&pi;<sub>0</sub></i> ("pi zero") of truly
	      null among all probesets.</li>
	    </ul>
	</li>
	<li>After havng estimated these three parameters for the
	  DenBoer expression dataset, do the same for the three negative
	  controls.</li>
      </ol>
    </div>


    <a href="javascript:sizeTbl2('block','sol_pi0')">View solution</a>| 
    <a href="javascript:sizeTbl2('none','sol_pi0')">Hide solution</a>
    <div id='sol_pi0' class='solution' style="overflow:hidden;display:none">
      <h3>Solutions</h3>
      <pre class='brush:r'>

## Define a function that takes as input a vector of p-values, 
## and estimatesthe m0, m1 and pi0 parameters;
estimate.pi0 <- function(p.values, lambda=0.5) {

    ## We will store the numbers in a list
    params <- list()
    params$n <- length(p.values) ## Total number of probesets
    params$n.lambda <- sum(p.values > lambda)  ## Number of probesets with p-value above lambda
    
    ## Extrapolate m0 from lambda and n.lambda, assuming homogeneous
    ## repartition of the trully null probesets
    params$m0.est <- min(params$n, params$n.lambda * 1/ (1 - lambda))
    
    ## Estimate m1
    params$m1.est <- params$n - params$m0.est
    
    ## Compute pi0
    params$pi0 <- params$m0.est / (params$m0.est + params$m1.est)
    
    ## Return the parameters
    return(params)
}


## Estimate parameters for denboer with lambda = 0.5, to check robustness of the parameters
print(estimate.pi0(denboer.welch$P.value, lambda=0.5))
## pi0 = 0.6298075

## Estimate parameters for denboer with lambda = 0.6
print(estimate.pi0(denboer.welch$P.value, lambda=0.6))
## pi0 = 0.6190818

## Estimate parameters for denboer with lambda = 0.7, to check robustness of the parameters
print(estimate.pi0(denboer.welch$P.value, lambda=0.7))
## pi0 = 0.6194558

## Estimate parameters for denboer with lambda = 0.3, to check robustness of the parameters
print(estimate.pi0(denboer.welch$P.value, lambda=0.3))
## pi0 = 0.6591828

################################################################
## Estimate the parameters for the negative controls, with lambda=0.5

## Normally distributed random numbers
print(estimate.pi0(rnorm.welch$P.value), lambda=0.5)
## 1 in our case, might differ in your case since results from random numbers

## Permutations of the expression matrix
print(estimate.pi0(perm.matrix.welch$P.value), lambda=0.5)
## 0.9986 in our case, might differ in your case since results from random numbers

## Permutations of the sample labels (cancer types)
print(estimate.pi0(perm.labels.welch$P.value), lambda=0.5)
## 1 in our case, might differ in your case since results from random numbers
      </pre>

      <h3>Interpretation</h3>

      <p>The pi0 parameter gives us an estimate of the proportion of
	truly null among a set of tested hypothesis. With the dataset
	from DenBoer, this parameter is relatively robust to the
	choice of the lambda parameter: &pi;0 takes values of 62%
	(&lambda;=0.7), 62% (&lambda;=0.6), 63% (&lambda;=0.5) or 66%
	(&lambda;=0.3), respectively. </p>

      <p>The two first negative controls (random numbers and permuted
	expression matrix, resp.) give the expected result: the
	estimated proportion of truly null hypotheses is 99% (by
	construction, we know that 100% of the sets correspond to null
	hypotheses). </p>

      <p>The third negative control (permuted labels) is less
	convincing: in our case, the procedure estimates that 88% of
	the probesets correspond to the null hypothesis, and that 2959
	genes are differentially expressed!  This reflects the
	inhomogeneous distribution of density values observed on the
	third density histogram of the previous section. Note that the
	values of the negative controls will vary for each trial,
	since these controls rely on random sampling.</p>
    </div>



    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='multitesting_why'></a>
    <h2>Why do we need to correct for multiple testing ?</h2>

    <p>A classical approach in statistical tests is to define <i>a
	priori</i> a level of significance, i.e. a threshold on the
      p-value. We will reject the null hypothesis if a test returns
      a lower p-value than the threshold, and accept it
      otherwise. Rejecting the null hypothesis means that we
      consider the test as significant. In the current case, this
      means that rejected hypotheses will correspond to
      differentially expressed genes.</p>

    <p>However, we are confronted to a problem: with the dataset from
      DenBoer (2009), we ran 22,283 tests in parallel. This means
      that, if we decide to use a classical p-value threshold of 1%,
      we accept, for each probeset, a risk of 1% to consider it
      significant whereas if follows the null hypothesis. Since this
      risk is multiplied 22,283 times, we expect an average of 223
      false positives for any dataset. </p>

    <div class='exo'>
      <h3>Exercise</h3>

      <p>Count the numbers of significant probesets in the four
	datasets described above.</p>
    </div>

    <a href="javascript:sizeTbl2('block','sol_p_value_fp')">View solution</a>| 
    <a href="javascript:sizeTbl2('none','sol_p_value_fp')">Hide solution</a>
    <div id='sol_p_value_fp' class='solution' style="overflow:hidden;display:none">
      <h3>Solutions</h3>

      <pre class='brush:r'>
print(threshold)	

## Count the number of probesets declared "positive" in the random normal dataset
sum(rnorm.welch$P.value <= threshold) 
## This returns 209 in our case (but can be different for you)

## Count the number of probesets declared "positive" in the permuted expression dataset
sum(perm.matrix.welch$P.value <= threshold) 
## This returns 172 in our case (but can be different for you)

## Count the number of probesets declared "positive" in the permuted labels dataset
sum(perm.labels.welch$P.value <= threshold) 
## This returns 254 in our case (but can be different for you)

## Count the number of probesets declared "positive" in DenBoer 2009 expression dataset
sum(denboer.welch$P.value <= threshold) 
## This returns 4456 positives, much more than any of the 3 controls above
			     </pre>

      <p>The negative controls clearly show that the classical way to
	control false positives by setting a threshold on the nominal
	p-value is problematic: when we set the significance threshold
	to 1%, we count ~200 false positives per analysis.
      </p>
      <p>In the next sections, we will try 3 alternative methods for
	controlling the number of false positives in such situations of
	multiple testing.</p>
    </div>

    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='e_value'></a>
    <h2>Expected number of false positives (e-value)</h2>


    <h3>Context</h3>

    <p>The E-value is the expected number of false positives. This is
    the simplest and most intuitive correction for multiple testing:
    given a threshold on the nominal p-value and the number of tests
    performed, we can easily compute the expected number of false
    positives.</p>

    <div class='exo'>
      <h3>Exercise</h3>

      <ol>
	<li>The result table of the function <tt>t.test.multi()</tt>
	  contains a column indicating the p-value. For the 4 datasets
	  (DenBoer + the three negative controls), use the column
	  "P.value" of the t.test.multi() result to compute the
	  <b>E-value</b> (<b>expected number of false positives</b>)
	  associated to each probeset. Compare it to the E-value
	  indicated in the t.test.multi() result.</li>
	<li>Count the number of probesets declared "positive" with a
	  threshold of 1% on E-value. </li>
      </ol>
    </div>

    <a href="javascript:sizeTbl2('block','sol_e_value')">View solution</a>| 
    <a href="javascript:sizeTbl2('none','sol_e_value')">Hide solution</a>
    <div id='sol_e_value' class='solution' style="overflow:hidden;display:none">
      <h3>Solutions</h3>

      <pre class='brush:r'>
## Compute the E-value from the P-value
E.value <- rnorm.welch$P.value * n.probesets

## Compare the E-value we just computed	with the one stored in the t.test.multi() result
E.value - rnorm.welch$E.value
sum(E.value != rnorm.welch$E.value) ## Count the number of probesets for which our E-value does not correspond the one computed by t.test.multi()

################################################################
## Count the number of probeset declared "positive" with an E-value threshold of 1%

print(threshold)

## In the random normal dataset
sum(rnorm.welch$E.value <= threshold)  ## This returns 0 in our case (but can be different for you)
min(rnorm.welch$E.value)  ## 3.65 in our case (can differ in your case)

## Count the number of probesets declared "positive" in the permuted expression dataset
sum(perm.matrix.welch$E.value <= threshold)  ## This returns 0 in our case (but can be different for you)
min(perm.matrix.welch$E.value) ## 0.29 in our case (can differ in your case)

## Count the number of probesets declared "positive" in the permuted labels dataset
sum(perm.labels.welch$E.value <= threshold) ## This returns 0 in our case (but can be different for you)
min(perm.labels.welch$E.value) ## 2.97 in our case (can differ in your case)

## Count the number of probesets declared "positive" in DenBoer 2009 expression dataset
sum(denboer.welch$E.value <= threshold)  ## This returns 873 positives, much more than any of the 3 controls above
min(denboer.welch$E.value) ## 5.29e-27 (this should be identical for you)
      </pre>

      <h3>Interpretation</h3>
      <p>A threshold at 1% on the E-value is very stringent: we did
	not get a single false positive for any of the three negative
	controls. This is a rather good news: it suggests that the 873
	probesets declared "positive" in the real expression set are
	likely to be correct (i.e. belong to the alternative
	hypothesis : <i>m<sub>hyper</sub> &ne; m<sub>TEL-ALL1</sub></i>.
      </p>
      <p>However, we have good reasons to suspect that this level of
	control is too stringent. Indeed, on the density plot we
	observed that the number of genes with p-value &le; 5%
	is <b>much</b> higher than the random expectation: we can see
	that the first bar of the histogram exceeds 6,000, whereas
	under the null hypothesis we would expect ~1,100 probesets per
	5% chunk of p-value density. In a first approximation, we
	could thus expect that the dataset contains ~5,500
	differentially expressed genes (this is a very rough
	approximation, we will see below how Storey and Tibshirani
	propose to estimate the proportions of null and alternative
	hypotheses in a multiple testing configuration).</p>
    </div>

    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='fwer'></a>
    <h2>Family-Wise Error Rate (FWER)</h2>
    

    <h3>Context</h3>
    
    <p>The <b>Family-Wise Error Rate (FWER)</b> is the probability to
      obtain at least one false positive by chance: P(FP >= 1), for a
      given threshold on p-value and taking into account the number of
      tests performed.</p>

    <div class='exo'>
      <h3>Exercise</h3>

      <ol>
	<li>For each one of the datasets analyzed above, add to the
	  t.test.multi() a column indicating the FWER.</li>
	<li>Count the number of probesets declared "positive" with a
	  threshold of 1% on FWER. Compare it with the number of false
	  positives when the control was exerted at the level of
	  nominal p-value, and E-value, respectively. </li>
	<li>Draw a plot comparing the E-value and FWER to the nominal
	  p-value, for all the probesets of the DenBoer dataset.</li>
      </ol>
    </div>

    <a href="javascript:sizeTbl2('block','sol_fwer')">View solution</a>| 
    <a href="javascript:sizeTbl2('none','sol_fwer')">Hide solution</a>
    <div id='sol_fwer' class='solution' style="overflow:hidden;display:none">
      <h3>Solutions</h3>

      <p>The reasoning for computing the FWER is the following. Let us
	consider a particular probest, which returns a
	p-value <i>p</i><i>t</i> (for example <i>p=1e-4</i>). Under
	the null hypothesis,</p>
      <ol>
	<li>The probability for one specific probeset to return a
	  false positive equals <i>p</i>. This is the <b>nominal</b>
	  p-value (i.e. the p-value attached to a given individual
	  test).
	</li>
	<li>For the same particular probeset, the
	  probability <b>not</b> to return a false positive is <i>1 -
	    p</i>.  
	</li>
	<li>If we perform <i>n</i> tests in parallel, the probability
	  for <b>not</b> returning a single false positive is 
	  <ul><i>P(FP = 0) = (1-p)<sup>n</sup> = (1-p) * (1-p) * ... * (1-p)</i> (n products)</ul>
	</li>
	<li>The probability of observing at least one false positve: 
	  <ul><i>P(FP &ge; 1) = 1 - P(FP =0) = 1 - (1-p)<sup>n</sup></i></sup>
	</li>
      </ol>

      <pre class='brush:r'>
## The computation of the raw formula is imprecise (cannot report values << 1e-16)
## rnorm.welch$FWER <- 1 - (1 - rnorm.welch$P.value)^n.probesets
rnorm.welch$FWER <- pbinom(q=0, size=n.probesets, prob=rnorm.welch$P.value, lower.tail=FALSE)
sum(rnorm.welch$FWER <= threshold) ##  0
min(rnorm.welch$FWER) ## 0.97 (this may vary between trials)

## perm.matrix.welch$FWER <- 1 - (1 - perm.matrix.welch$P.value)^n.probesets
perm.matrix.welch$FWER <- pbinom(q=0, size=n.probesets, prob=perm.matrix.welch$P.value, lower.tail=FALSE)
sum(perm.matrix.welch$FWER <= threshold) ##  0
min(perm.matrix.welch$FWER) ## 0.25

##perm.labels.welch$FWER <- 1 - (1 - perm.labels.welch$P.value)^n.probesets
perm.labels.welch$FWER <- pbinom(q=0, size=n.probesets, prob=perm.labels.welch$P.value, lower.tail=FALSE)
sum(perm.labels.welch$FWER <= threshold) ##  0
min(perm.labels.welch$FWER) ## 0.95

##denboer.welch$FWER <- 1 - (1 - denboer.welch$P.value)^n.probesets
denboer.welch$FWER <- pbinom(q=0, size=n.probesets, prob=denboer.welch$P.value, lower.tail=FALSE)
sum(denboer.welch$FWER <= threshold) ##  873
min(denboer.welch$FWER) ## 0 !

## Compare the E-value and FWER
x11(width=6, height=6)
plot(denboer.welch$E.value, denboer.welch$FWER, log="xy", panel.first=grid())

## Compare E-value and FWER to P-value
plot(denboer.welch$P.value, denboer.welch$E.value, log="xy", panel.first=grid(), pch=20)
lines(denboer.welch$P.value, denboer.welch$FWER, type="p", pch=1, col='grey')
      </pre>

    </div>

    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='q_value'></a>
    <h2>Controlling the False Discovery Rate (FDR) with the q-value</h2>


    <h3>Context</h3>

    <p>The <b>q-value</b> estimates the <b>False discovery rate</b>
      (<b>FDR</b>), i.e. the expected proportion of false
      positives <i>among the cases declared positives</i>. This makes
      an essential distinction with the <b>False Positive Rate</b>
      (<b>FPR</b>), which indicates the proportion of false positves
      among all the tests performed. </p>

    <p>Under the null hypothesis, the FDR can be estimated in the
      following way (hochberg and Benjamini):
      <ol>
	<li>sort elements by increasing p-value</li>
	<li>for each rank <i>i</i> of the sorted list, compute <i>q(i) = &pi;0 * Pval * m / i</i></li>
	<li>choose a given significance level <i>q*</i> (in our case,
	  let us chose q*= 1%). Let <i>k</i> be the largest <i>i</i> for
	  which <i>q(i) &le; q*</i>. Reject all
	  hypotheses <i>H<sub>(i)<sub></i> with <i>i = 1, 2, ...,
	    k</i>.</li>
      </ol>
    </p>

    <div class='exo'>
      <h3>Exercise</h3>

      <ol>
	<li>Compute the q-value for the DenBoer dataset and for the
	  three control tests described above.</li>
	<li>Generate a plot comparing the nominal p-value (abscissa)
	  with the different corrections (E-value, FWER,
	  q-value).</li>
	<li>Count the number of probesets declared significant at a
	  level of 1%, and compare it to the numbers of positives
	  detected above, when the control was performed at the level of
	  the p-value, E-value or FWER. </li>
      </ol>
    </div>

    <a href="javascript:sizeTbl2('block','sol_q_value')">View solution</a>| 
    <a href="javascript:sizeTbl2('none','sol_q_value')">Hide solution</a>
    <div id='sol_q_value' class='solution' style="overflow:hidden;display:none">
      <h3>Solutions</h3>

      <pre class='brush:r'>
## Sort the Welch test result table set by increasing p-values
denboer.welch.sorted <- denboer.welch[order(denboer.welch$P.value, decreasing=FALSE),]

## Check that the lowest p-values come on top of the sorted table ...
head(denboer.welch.sorted)

## ... and that the highest p-values come on the bottom of the sorted table
tail(denboer.welch.sorted)

## Add a column with the rank
denboer.welch.sorted$i <- 1:nrow(denboer.welch.sorted)

## Use the function defined above to estimate pi0
denboer.param <- estimate.pi0(denboer.welch$P.value, lambda=0.5)

## Compute the q.value
denboer.welch.sorted$q.value <- denboer.param$pi0 * denboer.welch.sorted$P.value * n.probesets / denboer.welch.sorted$i


## Draw a plot comparing the nominal p-value and the q-value
plot(denboer.welch.sorted$P.value, denboer.welch.sorted$q.value, panel.first=grid(), xlim=c(0,1), ylim=c(0,1), pch=2, xlab="Nominal p-value", ylab="q-value", col="orange")
abline(a=0, b=1, col="violet") ## Draw the diagonal
abline(h=denboer.param$pi0, col='blue', lwd=2) ## Draw the pi0 estimtate

## Draw a plot comparing the nominal p-value and the different multiple testing corrections.
## Use logarithmic axes to highlight low values (high significance).
plot(denboer.welch.sorted$P.value, denboer.welch.sorted$E.value, panel.first=grid(), pch=1, xlab="Nominal p-value", ylab="Multiple testing corrected", log="xy", xlim=c(min(denboer.welch.sorted$P.value), 1), col="grey", main="multiple testing corrections")
abline(a=0, b=1, col="violet") ## Mark the diagonal
abline(h=1, col='blue', lwd=2) ## Mark the max possible probability value (1)
lines(denboer.welch.sorted$P.value, denboer.welch.sorted$FWER, type="p", pch=20)
lines(denboer.welch.sorted$P.value, denboer.welch.sorted$q.value, type="p", pch=2, col="orange")
legend("bottomright", legend=c("e-value", "FWER", "q-value"), pch=c( 1, 20, 2), col=c("grey", "black", "orange"))
abline (h=threshold, col="red", lwd=2) ## Mark the significance threshold

## Count number of probesets declared significant with different control types
threshold <- 0.01
sum(denboer.welch.sorted$P.value <= threshold) ## 4456 probesets
sum(denboer.welch.sorted$E.value <= threshold) ## 873 probesets
sum(denboer.welch.sorted$FWER <= threshold) ## 873 probesets
sum(denboer.welch.sorted$q.value <= threshold) ## 3355 probesets

## Recall the first estimate of m1 (number of truly alternative probesets)
print(denboer.param$m1) ## 8249
      </pre>
    </div>

    <a href='#contents'><font size=-2>[back to contents]</font></a>
    <hr><a name='biblio'></a>
    <h2>Bibliographic references</h2>
    
    <ol>
      <li><a name='benjamini_hochberg_1995'></a>Benjamini Y, Hochberg
	Y. (1995). Controlling the false discovery rate: a practical and
	powerful approach to multiple testing. JRStatistSocB 57(1):
	289-300.
      </li>

      <li><a name='storey_tibshirani_2003'></a> Storey JD, Tibshirani
	R. (2003). Statistical significance for genomewide
	studies. Proc Natl Acad Sci U S A 100(16): 9440-9445.
	[<a href='http://www.ncbi.nlm.nih.gov/pubmed/12883005'>PMID 12883005</a>]
	[<a href='http://www.pnas.org/content/100/16/9440.long'>free article</a>]
      </li>

      <li>
	<a name='denboer_2009'></a> Den Boer ML, van Slegtenhorst M,
	De Menezes RX, Cheok MH, Buijs-Gladdines JG, Peters ST, Van
	Zutven LJ, Beverloo HB, Van der Spek PJ, Escherich G et
	al. 2009. A subtype of childhood acute lymphoblastic leukaemia
	with poor treatment outcome: a genome-wide classification
	study. Lancet Oncol 10(2): 125-134.
      </li>
    </ol>

    <hr>
    <address>
      Denis Puthier (<a target='_blank' href='http://tagc.univ-mrs.fr/tagc/index.php/research/developmental-networks/d-puthier'>http://tagc.univ-mrs.fr/tagc/index.php/research/developmental-networks/d-puthier</a>)</li>
      & <a target='_blank' href='http://jacques.van-helden.perso.luminy.univmed.fr/'>Jacques van Helden</a> (<a target='_blank' href='http://tagc.univ-mrs.fr/'>TAGC</a>, Aix-Marseille Universit&eacute;).
    </address>
</html>


