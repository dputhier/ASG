\subsection{Abbreviations}\label{abbreviations}

\begin{longtable}[c]{@{}ll@{}}
\toprule\addlinespace
Abbrev & Meaning
\\\addlinespace
\midrule\endhead
ALL & acute lymphoblastic leukemia
\\\addlinespace
AML & acute myeloblastic leukemia
\\\addlinespace
DEG & differentiall expressed genes
\\\addlinespace
GEO & Gene Expression Omnibus database
\\\addlinespace
KNN & K-nearest neighbours
\\\addlinespace
LDA & linear discriminant analysis
\\\addlinespace
QDA & quadratic discriminant analysis
\\\addlinespace
PCA & principal component anlaysis
\\\addlinespace
LOO & leave-one-out
\\\addlinespace
CV & cross-validation
\\\addlinespace
\bottomrule
\end{longtable}

\subsection{R configuration}\label{r-configuration}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Open a terminal
\item
  Start R
\end{itemize}

We first need to define the URL of the course (dir.base), from which we
will download some pieces of {R} code and the data sets to analyze.

\texttt{\{r\} \#\# Specify the URL of the base for the course dir.base \textless{}- 'http://pedagogix-tagc.univ-mrs.fr/courses/statistics\_bioinformatics'}

The following command loads a general configuration file, specifying the
input directories (data, {R} utilities) and creating output directories
on your computer (dir.results, dir.figures)..

``` \{r\} \#\# Load the general configuration file
source(file.path(dir.base, `R-files', `config.R'))

setwd(dir.results) print(paste(``Result directory'', dir.results)) ```

\texttt{\{r\} \#\# Check the requirement for some packages pkg \textless{}- "qvalue" if (!suppressPackageStartupMessages(require(pkg, quietly=TRUE, character.only = TRUE))) \{   source("http://bioconductor.org/biocLite.R")   biocLite();   biocLite(pkg) \}}

\subsection{Goal of this tutorial}\label{goal-of-this-tutorial}

In this tutorial, we will put in practice some fundamental concepts of
supervised classification.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Distinction between unsupervised (clustering) and supervised
  classification.
\item
  Steps of an analysis: training, testing and prediction.
\item
  Evaluation of the classification: cross-validation ({CV}),
  leave-one-out ({LOO}).
\item
  The problem of over-dimensionality and the risk of overfitting
\item
  Feature selection methods
\end{enumerate}

\subsection{Study case}\label{study-case}

For the practical, we will use a cohort comprized of 190 samples from
patients suffering from Acute Lymphoblastic Leukemia (\textbf{ALL}) from
DenBoer (2009). The raw data has previously been retrieved from the Gene
Expression Omnibus (\textbf{GEO}) database
(\url{http://www.ncbi.nlm.nih.gov/geo/}).

\subsubsection{Data loading}\label{data-loading}

We can now load the profile table and check its dimensions. Beware: this
expression matrix weights 20Mb. The download can thus take time,
depending on your internet connection.

``` \{r\} \#\# Define the location of data directory and file containing
expression profiles dir.denboer \textless{}- file.path(dir.base, `data',
`gene\_expression',`denboer\_2009') file.expr.table \textless{}-
file.path(dir.denboer, `GSE13425\_Norm\_Whole.tab')

\subsection{Load the expression profiles (one row per gene, one column
per
sample).}\label{load-the-expression-profiles-one-row-per-gene-one-column-per-sample.}

expr.matrix \textless{}- read.table(file.expr.table, sep = ``\t'', head
= T, row = 1) print(dim(expr.matrix)) \#\# Should give this: 22283 190
```

Once the whole data set has been loaded, the data frame ``expr.matrix''
should contain 22,283 rows (genes) and 190 columns (samples).

We can now load the sample descriptions (``phenoData'').

``` \{r\} \#\# Load the table describing each sample \#\# (one row per
sample, one column per description field). pheno \textless{}-
read.table(file.path(dir.denboer, `phenoData\_GSE13425.tab'), sep=`\t',
head=TRUE, row=1) dim(pheno) \#\# {[}1{]} 190 4

\subsection{We can check the content of the phenoData table
by}\label{we-can-check-the-content-of-the-phenodata-table-by}

\subsection{consulting its column
names.}\label{consulting-its-column-names.}

names(pheno) ```

The column \emph{Sample.title} indicates the cancer subtype
corresponding to each sample.

Remark: the ``pheno'' variable contains the information defined as
``phenotypic'' for general analysis of microarrays. However, in the case
of DenBoer data set, the different classes of ALL are characterized by
genotypic characteristics (the mutation that caused the leukemia) rather
than by phenotypic traits.

We can count the number of samples per subtype, and display them by
decreasing group size.

\texttt{\{r\} \#\# Print the number of samples per cancer type print(data.frame("n"=sort(table(pheno\$Sample.title),decreasing=T)))}

For the sake of visualization, we will define short labels corresponding
to each ALL subtype, and assign these short labels to the samples.

``` \{r\} \#\# Define an abbreviated name for each canceer subtype \#\#
(will be used later visualization on the plots) group.abbrev
\textless{}- c( `BCR-ABL + hyperdiploidy'=`Bch', `BCR-ABL'=`Bc',
`E2A-rearranged (E)'=`BE', `E2A-rearranged (E-sub)'=`BEs',
`E2A-rearranged (EP)'=`BEp', `MLL'=`BM', `T-ALL'=`T', `TEL-AML1 +
hyperdiploidy'=`Bth', `TEL-AML1'=`Bt', `hyperdiploid'=`Bh', `pre-B
ALL'=`Bo' ) sample.subtypes \textless{}- as.vector(pheno\$Sample.title)
sample.labels \textless{}- group.abbrev{[}sample.subtypes{]}
names(sample.labels) \textless{}- names(expr.matrix)

\subsection{Check the label for a random selection of 10
samples.}\label{check-the-label-for-a-random-selection-of-10-samples.}

\subsection{Each run should give a different
result}\label{each-run-should-give-a-different-result}

sample(sample.labels, size=10) ```

We can also define group-specific colors and assign them to samples.

``` \{r\} \#\# Define group-specific colors group.colors \textless{}- c(
`BCR-ABL + hyperdiploidy'=`cyan', `BCR-ABL'=`black', `E2A-rearranged
(E)'=`darkgray', `E2A-rearranged (E-sub)'=`green', `E2A-rearranged
(EP)'=`orange', `MLL'=`\#444400', `T-ALL'=`violet', `TEL-AML1 +
hyperdiploidy'=`\#000066', `TEL-AML1'=`darkgreen', `hyperdiploid'=`red',
`pre-B ALL'=`blue' )

\subsection{Assign group-specific colors to
patients}\label{assign-group-specific-colors-to-patients}

sample.colors \textless{}-
group.colors{[}as.vector(pheno\$Sample.title){]} names(sample.colors)
\textless{}- names(expr.matrix) \#\# Show some random examples of color
assignations print(as.data.frame(sample(sample.colors,size=10))) ```

\subsection{Reducing the dimensionality of the data - feature
selection}\label{reducing-the-dimensionality-of-the-data---feature-selection}

An recurrent problem with microarray data is the large dimensionality of
the data space. The dataset we are analyzing contains 190 samples (the
{subjects}), each characterized by 22,283 gene expression values (the
{variables}).

The number of variables (also called {features}, the genes in our case)
thus exceeds by far the number of objects (samples in this case). This
sitation is qualified of {over-dimensionality}, and poses serious
problems for classification.

In unsupervised classification (clustering), the relationships between
objects will be affected, since a vast majority of the features are
likely to be uninformative, but will however contribute to the computed
(dis)similarity metrics (whichever metrics is used). Overdimensionality
will somewhat mask the signal (biologically relevant relationships
between gene groups) with noise. In supervised classification, the
effect may be even worse: a program will be trained to recognize classes
on the basis of supriousn differences found in any combination of the
input variables.

It is thus essential, for both unsupervised and unsupervised
classification, to perform some {feature selection} before applying the
actual classification.

\subsubsection{An arbitrary pair of
genes}\label{an-arbitrary-pair-of-genes}

To get some feeling about the data, we will compare the expression
levels of two genes selected in an arbitrary way (resp. $236^{th}$ and
$1213^{th}$ rows of the profile table).

\texttt{\{r fig.width=8, fig.height=8\} \#\# Plot the expression profiles of two arbitrarily selected genes g1 \textless{}- 236 g2 \textless{}- 1213 x \textless{}- as.vector(as.matrix(expr.matrix{[}g1,{]})) y \textless{}- as.vector(as.matrix(expr.matrix{[}g2,{]})) plot(x,y,        col=sample.colors,        type="n",        panel.first=grid(col="black"),             main="Den Boer (2009), two arbitrary genes",        xlab=paste("gene", g1), ylab=paste("gene", g2)) text(x, y,labels=sample.labels,col=sample.colors,pch=0.5) legend("topright",col=group.colors,           legend=names(group.colors),pch=1,cex=0.6,bg="white",bty="o") dev.copy2pdf(file="two\_arbitrary\_genes\_plot.pdf", width=8, height=8)}

Each dot corresponds to one sample. Since the genes were initially
selected at random, we don't expect them to be particularly good at
discriminating the different subtypes of ALL. However, we can already
see some emerging patterns: the T-ALL (labelled ``T'') show a trends to
strongly express the $236^{th}$ gene, and, to a lesser extent, several
hyperdiploid B-cells (label ``Bh'') show a high expression of the
$1213^{th}$ gene. Nevertheless, it would be impossible to draw a
boundary that would perfectly separate two subtypes in this plot.

\subsubsection{Variance filter}\label{variance-filter}

In order to reduce the dimensionality of the data set, we will sort the
genes according to their variance, and retain a defined number of the
top-ranking genes. Note that this ranking is a very rudimentary way to
select a subset of genes. Indeed, nothing guarantees us that the genes
showing the largest inter-individual fluctuations of expression have
anything related to cancer type. In further sections, we will present
alternative methods of variable ordering, which will explicitly take
into account the inter-group variance.

The {R} function apply() can be used to apply any function to each row
(or alternatively each column) of a data table. We set the second
argument (margin) to 1, thereby indicating that the function (third
argument) must be applied to each row of the input table (first
argument).

\paragraph{Variance per sample}\label{variance-per-sample}

``` \{r fig.width=7, fig.height=5\} \#\# Compute sample-wise variance
var.per.sample \textless{}- apply(expr.matrix, 2, var)
head(var.per.sample)

\subsection{Inspect the distribution of sample-wise
variance}\label{inspect-the-distribution-of-sample-wise-variance}

hist(var.per.sample, breaks=50, col=``\#BBBBFF'')

\subsection{Store the figure in a pdf
file}\label{store-the-figure-in-a-pdf-file}

dev.copy2pdf(file=file.path(dir.figures,
``denboer2009\_variance\_per\_sample.pdf'')) ```

\paragraph{Variance per gene}\label{variance-per-gene}

``` \{r fig.width=7, fig.height=5\} \#\# Compute gene-wise variance
var.per.gene \textless{}- apply(expr.matrix, 1, var)

\subsection{Inspect the distribution of gene-wise
variance}\label{inspect-the-distribution-of-gene-wise-variance}

hist(var.per.gene, breaks=100, col=``\#BBFFDD'')
dev.copy2pdf(file=file.path(dir.figures,
``denboer2009\_variance\_per\_gene.pdf''), width=7,height=5) ```

We notice that gene-wise variances have a wide dispersion. the histogram
shows a right-skewed distribution, with a long tail due to the presence
of a few genes with high variance. If we sort genes by decreasing
variance, we can see that there is a strong difference between the top
and the bottom of the list.

``` \{r\} \#\# Sort genes per decreasing variance genes.by.decr.var
\textless{}- sort(var.per.gene,decreasing=TRUE)

\subsection{Print the 5 genes with highest
variance}\label{print-the-5-genes-with-highest-variance}

head(genes.by.decr.var)

\subsection{Print the 5 genes with lowest
variance}\label{print-the-5-genes-with-lowest-variance}

tail(genes.by.decr.var) ```

We can then select an arbitrary number of top-ranking genes in the list.

``` \{r\} \#\# Select the 30 top-ranking genes in the list sorted by
variance. \#\# This list of genes will be used below as training
variables for \#\# supervised classification. top.nb \textless{}- 20
\#\# This number can be changed for testing genes.selected.by.var
\textless{}- names(genes.by.decr.var{[}1:top.nb{]})

\subsection{Check the names of the first selected
genes}\label{check-the-names-of-the-first-selected-genes}

head(genes.selected.by.var, n=top.nb) ```

\subsection{Assigning ranks to genes according to some sorting
criterion}\label{assigning-ranks-to-genes-according-to-some-sorting-criterion}

In the next sections, we will compare methods for selecting genes on the
basis of different criteria (variance, T-test, ANOVA test, step-forward
procedure in Linear Discriminant Analysis). For this purpose, we will
create a table indicating the values and the rank of each gene (rows of
the table) according to each selection criterion (columns).

\subsubsection{Ranking genes by inter-sample
variance}\label{ranking-genes-by-inter-sample-variance}

We first instantiate this table with two columns indicating the name and
variance of each gene. We will then add columns indicating the rank of
each gene according to its variance, and to other criteria.

\texttt{\{r\} \#\# Create a data frame to store gene values and ranks  \#\# for different selection criteria. gene.ranks \textless{}- data.frame(var=var.per.gene) head(gene.ranks)}

The R function \emph{rank()} assigns a rank to each element of a list of
values, by increasing order. We will use a trick to assign ranks by
decreasing order: compute the rank of \emph{minus variance}. The option
\emph{ties} defines the way to treat equal values.

\texttt{\{r\} \#\# Beware, we rank according to minus variance, because  \#\# we want to associate the lowest ranks to the highest variances gene.ranks\$var.rank \textless{}- rank(-gene.ranks\$var, ties.method='random') head(gene.ranks, n=10)}

In this table, genes are presented in their original order (i.e.~their
order in the microarray dataset). We can reorder them in order to print
them by decreasing variance, and check five top and bottom lines of the
sorted table.

``` \{r\} \#\# Check the rank of the 5 genes with highest and lowest
variance, resp. gene.ranks{[}names(genes.by.decr.var{[}1:5{]}),{]}

\subsection{Print the bottom 5 genes of the variance-sorted
table}\label{print-the-bottom-5-genes-of-the-variance-sorted-table}

gene.ranks{[}names(tail(genes.by.decr.var)),{]} ```

We have no specific reason to think that genes having a high variance
will be specially good at discriminating ALL subtypes. Indeed, a high
variance might \emph{a priori} either reflect cell-type specificities,
or unrelated differences between samples resulting from various effects
(individual patient genomes, transcriptome, condition, \ldots{}). For
the sake of curiosity, let us plot samples on a XY plot where the
abcsissa represents the top-raking, and the ordinate the second
top-ranking gene in the variance-ordered list.

``` \{r fig.width=8, fig.height=8\} \#\# Select the gene withthe highest
variance maxvar.g1 \textless{}- names(genes.by.decr.var{[}1{]})
print(maxvar.g1)

\subsection{Select the second gene withthe highest
variance}\label{select-the-second-gene-withthe-highest-variance}

maxvar.g2 \textless{}- names(genes.by.decr.var{[}2{]}) print(maxvar.g2)

\subsection{Plot the expression profiles of the two genes with highest
variance}\label{plot-the-expression-profiles-of-the-two-genes-with-highest-variance}

x \textless{}- as.vector(as.matrix(expr.matrix{[}maxvar.g1,{]})) y
\textless{}- as.vector(as.matrix(expr.matrix{[}maxvar.g2,{]})) plot(x,y,
col=sample.colors, type=`n', panel.first=grid(col=`black'), main=``2
genes with the highest variance'', xlab=paste(`gene', maxvar.g1),
ylab=paste(`gene', maxvar.g2)) text(x,
y,labels=sample.labels,col=sample.colors,pch=0.5)
legend(`topright',col=group.colors,
legend=names(group.colors),pch=1,cex=0.6,bg=`white',bty=`o')
dev.copy2pdf(file=``denboer2009\_2\_maxvar\_genes.pdf'', width=8,
height=8) ```

Somewhat surprizingly, the plot shows that, despite the roughness of our
ranking criterion (variance across all cancer types), the two
top-ranking already discriminate several sample types.

\begin{itemize}
\item
  The expression level of CD9 (abcsissa of the plot) gives a pretty good
  separation between some cancer types with low expression levels (T and
  Bt), and some other types characterized by a high expression level
  (Bh, BEs, BEp). Cancer type Bo is however dispersed over the whole
  range of CD9 expression.
\item
  IL23A clearly separates the subtype ``T'' (high levels) from all other
  cancer subtypes (low levels).
\end{itemize}

\subsubsection{Variable ordering by Welch t-test (2-groups
test)}\label{variable-ordering-by-welch-t-test-2-groups-test}

The T-test tests the hypothesis of equality between the means of two
populations. We can use the Welch version of the t-test (which assumes
that groups can have different variances) in order to select genes
differentially expressed between two goups of samples.

\[m_1 = m_2\]

We thus need to define two groups of samples (multi-group comparisons
will be treated by ANOVA). For the dataset on ALL, we have several
subtypes of cancer. we will perform pairwise comparisons of mean for a
selection of subtypes. In each case, a given subtype (e.g.~T-cells) will
be compared to all other subtypes pooled together.

We will successvely run Welch's t-test for the main subtypes (``Bo'',
``Bh'', ``Bt'', ``T'').

\[H_0:; m_{Bo} = m_{others}\]

\[H_0:; m_{Bh} = m_{others}\]

\[H_0:; m_{Bt} = m_{others}\]

\[H_0:; m_{T} = m_{others}\]

In each case, apply the test in to each gene, using the function
t.test.multi() defined in the {R} utilities of this course.

``` \{r\} \#\# Load a utility to apply Welch test on each row of a table
source(file.path(dir.util, ``util\_student\_test\_multi.R''))

\subsection{Define a vector indicating whether each
sample}\label{define-a-vector-indicating-whether-each-sample}

\subsection{belongs to the subtype of interest (e.g. ``Bo'') or
not.}\label{belongs-to-the-subtype-of-interest-e.g.-bo-or-not.}

group.of.interest \textless{}- ``Bo'' one.vs.others\textless{}-
sample.labels one.vs.others{[}sample.labels != group.of.interest{]}
\textless{}- ``o'' print(table(one.vs.others)) ```

To apply Welch test to each gene of the table, we will use our custom
function \emph{t.test.multi()} (see the previous practical on selection
of differentialy expressed genes, {DEG}).

\texttt{\{r\} \#\# Test the mean equality between Bo subtype  \#\# and all other subtypes  welch.one.vs.others \textless{}- t.test.multi(expr.matrix,                                      one.vs.others,                                     volcano.plot = FALSE)}

This method returns a table with various statistics about DEG.

\texttt{\{r\} head(welch.one.vs.others)}

We will compute a new rank, based on the (decreasing) significance of
the Welch test.

``` \{r\} \#\# Update the gene rank table test.name \textless{}-
paste(group.of.interest, `.vs.others.sig', sep='')

\subsection{Add a column with
significances}\label{add-a-column-with-significances}

gene.ranks{[},test.name{]} \textless{}- welch.one.vs.others\$sig

\subsection{Add a column with significance
ranks}\label{add-a-column-with-significance-ranks}

gene.ranks{[},paste(test.name, ``.rank'', sep=``''){]} \textless{}-
rank(-welch.one.vs.others\$sig, ties.method=`random')

\subsection{Apply the Welch test for the 3 other majority
groups}\label{apply-the-welch-test-for-the-3-other-majority-groups}

for (group.of.interest in c(``Bh'', ``Bt'', ``T'', ``Bo'')) \{
print(paste(``Selecting differentially expressed genes for'',
group.of.interest, ``versus others'')) one.vs.others \textless{}-
sample.labels one.vs.others{[}sample.labels != group.of.interest{]}
\textless{}- ``o''

\begin{verbatim}
## Test the mean equality between Bo subtype
## and all other subtypes
welch.one.vs.others <- t.test.multi(expr.matrix,
                                          one.vs.others,
                                          volcano.plot = FALSE)

## Store the volcano plot
dev.copy2pdf(file=file.path(dir.figures,
                            paste(sep="", "denboer2009_welch_",
                                 group.of.interest,"_vs_others_volcano.pdf")))

## Update the gene rank table
test.name <- paste(group.of.interest, '.vs.others.sig', sep='')
gene.ranks[,test.name] <- welch.one.vs.others$sig
gene.ranks[,paste(test.name, ".rank", sep="")] <-
  rank(-welch.one.vs.others$sig, , ties.method='random')
\end{verbatim}

\}

\subsection{Check the resulting gene
table}\label{check-the-resulting-gene-table}

head(gene.ranks)
head(gene.ranks{[}order(gene.ranks\$T.vs.others.sig.rank),{]})
tail(gene.ranks{[}order(gene.ranks\$T.vs.others.sig.rank),{]})

\subsection{Store the gene rank table in a text file (tab-separated
columns)}\label{store-the-gene-rank-table-in-a-text-file-tab-separated-columns}

write.table(gene.ranks, file=file.path(dir.results,
`DenBoer\_gene\_ranks.tab'), sep=`\t', quote=F, col.names=NA) ```

``` \{r fig.width=12, fig.height=12\} \#\# Plot variance against
significance of the Welch test \#\# for the 4 different groups
plot(gene.ranks{[},c(``var'', ``Bo.vs.others.sig'',
``Bh.vs.others.sig'', ``Bt.vs.others.sig'', ``T.vs.others.sig''){]},
col=``grey'')

\subsection{Plot ranks of variance against significance of the Welch
test}\label{plot-ranks-of-variance-against-significance-of-the-welch-test}

\subsection{for the 4 different
groups}\label{for-the-4-different-groups}

plot(gene.ranks{[},c(``var.rank'', ``Bo.vs.others.sig.rank'',
``Bh.vs.others.sig.rank'', ``Bt.vs.others.sig.rank'',
``T.vs.others.sig.rank''){]}, col=``grey'')
dev.copy2pdf(file=``denboer2009\_rank\_vs\_rank\_plots.pdf'', width=12,
height=12) ```

An obvious observation: the gene-wise variance and the 4 results of
Welch tests (one cancer type against all other types) return very
different values, and assign gene ranks in completely different ways.
This is not surprizing, the variance and the 4 significances indicate
different properties of the genes.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  the variance is a global property of the genes, and has not
  specifically to be related to a given subtype of cancer;
\item
  the other ranking criteria should rank genes in different orders by
  construction, since each one was based on a different group-specific
  DEG test.
\end{itemize}

Gene selection will thus have to be adapted to the specific purpose of
the classification.

In summary, so far we defined 5 different gene sorting criteria that
could be used to select subsets of features (genes) in order to train a
classifier: one general criterion (variance), and 4 group-specific
criteria (Welch test results). A priori, we would expect to achieve
better results with group-specific feature selection.

\subsubsection{ANOVA-based variable ordering (multiple
subtypes)}\label{anova-based-variable-ordering-multiple-subtypes}

The {ANOVA} test allows to select genes whose inter-group variation is
significantly higher than te intra-group variation. ANOVA can be thought
of as generalization of the Welch test. Welch test (and Student test)
are used to test equality between two groups (for example one subtype of
interest and all the other subtypes), whereas ANOVA simultaneously tests
the equality between multiple groups. We could thus use ANOVA to
establish a general ranking criterion that would select the genes
showing higest differences between ALL subtypes, without specifying a
priori which particular subtypes have to be different.

In a first time, we will apply the ANOVA test to one arbitrarily
selected gene. We will then see how to run this test on each row of the
expression matrix.

Some remarks about the implementation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In contrast with the Welch test, which was a 2-groups test, ANOVA can
  be used to compare multiple groups in a single analysis. For ANOVA, we
  will use the original sample labels (with all the ALL subtypes
  explicitly named), rather than the \emph{one.vs.other} vector that we
  created for 2-groups analysis.
\item
  We will runa single-factor anova, with gene expression as values, and
  sample labels as grouping.
\item
  The R methods \emph{aov()} and \emph{anova()} take as input a data
  frame with the values (gene expression values) in the first column,
  and the groupings (sample labels) in the second one.
\item
  \emph{R} proposes two methods for the ANOVA test. The \emph{aov()}
  function automatically fits the linear model and runs the anova test.
  However it is conceived for balanced groups, which is not our case
  (some of the ALL subtypes have very few samples) gives a warning
  ``Estimated effects must be unbalanced''
\end{enumerate}

```\{r\} \#\# Build a data frame with gene expression values in the
first column, \#\# and sample labels in the second column.

g \textless{}- 1234 \#\# Select an arbitrary gene g.expr \textless{}-
unlist(expr.matrix{[}g,{]}) \#\# Select the expression profile for this
gene g.for.anova \textless{}- data.frame(``expr''=g.expr,
``group''=sample.labels)

\subsection{Run the aov() method to check the
warnings}\label{run-the-aov-method-to-check-the-warnings}

g.aov.result \textless{}- aov(formula = expr \textasciitilde{} group,
data = g.for.anova) print(g.aov.result)

\subsection{We thus try the indirect approach: fit a linear model and
run anova on
it.}\label{we-thus-try-the-indirect-approach-fit-a-linear-model-and-run-anova-on-it.}

g.anova.result \textless{}- anova(lm(formula = expr \textasciitilde{}
group, data = g.for.anova)) print(g.anova.result)

\subsection{Extract the p-value from the ANOVA
result}\label{extract-the-p-value-from-the-anova-result}

attributes(g.anova.result) pval \textless{}-
as.numeric(unlist(g.anova.result){[}``Pr(\textgreater{}F)1''{]})
print(pval)

\subsection{Compute the e-value from this
p-value}\label{compute-the-e-value-from-this-p-value}

eval \textless{}- pval * nrow(expr.matrix) print(eval)

\subsection{Summarise the result in a
vector}\label{summarise-the-result-in-a-vector}

g.anova.summary \textless{}- c(``g''=g,
``name''=row.names(expr.matrix{[}g,{]}), ``pval''=pval, ``eval''=eval,
``sig''=-log(eval, base=10)) print(as.data.frame(g.anova.summary)) ```

\paragraph{How to interpret the ANOVA result
?}\label{how-to-interpret-the-anova-result}

\paragraph{Running ANOVA on each gene of the expression
table}\label{running-anova-on-each-gene-of-the-expression-table}

{We currently leave this as an exercise.}

\subsection{Principal Component
Analysis}\label{principal-component-analysis}

Before starting the proper process of supervised classification, we can
apply a method called Principal Component Analysis ({PCA}) to evaluate
the repartition of the information between the mutiple variables, and to
inspect the ``intrinsic'' structure of the data, i.e.~the structure
inherent to the numbers in the data table, irrespective of the labels
(cancer subtypes) attached to the various samples.

\subsubsection{Purpose of PCA}\label{purpose-of-pca}

We can do the exercise of extending this 2-dimensional plot to a
3-dimensional plot, where the third dimension represents the expression
level of a third gene. With an effort of imagination, we can mentally
extend this 3D plot to a 4-dimensional plot, where each dimension would
represent a different gene. It is likely that the groups of genes will
progressively become more separated as the number of dimension
increases. However, for the sake of graphical representation, it is
difficult to work with more than 2 (or at most 3) dimensions.

The purpose of {Principal Component Analysis} ({PCA}) is to capture the
largest part of the variance of a data set with a minimal number of
dimensions.

\subsubsection{Applying PCA transformation with
stats::prcomp()}\label{applying-pca-transformation-with-statsprcomp}

The {R} method stats::prcomp() performs a PCA transformation of an input
table. We can feed it with the expression table, but we need to
transpose it first (using the {R} function t()), in order to provide our
objects of interest (the samples) as rows, and the variables (genes) as
columns.

``` \{r\} \#\# load the stats library to use the princomp() and prcomp()
function library(stats)

\subsection{Perform the PCA
transformation}\label{perform-the-pca-transformation}

expr.prcomp \textless{}- prcomp(t(expr.matrix),cor=TRUE)

\subsection{Analyze the content of the prcomp
result:}\label{analyze-the-content-of-the-prcomp-result}

\subsection{the result of the method prcomp() is an
object}\label{the-result-of-the-method-prcomp-is-an-object}

\subsection{belonging to the class
``prcomp''}\label{belonging-to-the-class-prcomp}

class(expr.prcomp) ```

``` \{r\} \#\# Get the field names of the prcomp objects
names(expr.prcomp)

\subsection{Get the attributes of the prcomp
objects}\label{get-the-attributes-of-the-prcomp-objects}

attributes(expr.prcomp) ```

\subsubsection{Repartition of the standard deviation along the
components}\label{repartition-of-the-standard-deviation-along-the-components}

A first information is the repartition of the variance (or its squared
root, the standard deviation) between the components, which can be
displayed on a plot.

\texttt{\{r fig.width=7, fig.height=5\} plot(expr.prcomp, main='Den Boer (2009),       Variance  per component', xlab='Component', col="\#BBDDFF")}

The standard deviation barplot
(Figure\textasciitilde{}\ref{DenBoer_pca_variance}) highlights that the
first component captures more or less twice as much standard deviation
of the whole dataset as the second one. We can measure the relative
importance of the standard deviations.

``` \{r\} \#\# Get the standard deviation and variance per principal
component sd.per.pc \textless{}- expr.prcomp\$sdev var.per.pc
\textless{}- sd.per.pc\^{}2

\subsection{Display the percentage of total variance explained by
each}\label{display-the-percentage-of-total-variance-explained-by-each}

sd.per.pc.percent \textless{}- sd.per.pc/sum(sd.per.pc)
var.per.pc.percent \textless{}- var.per.pc/sum(var.per.pc)
barplot(var.per.pc.percent{[}1:10{]}, main=`Den Boer (2009), Percent of
variance per component', xlab=`Component', ylab=`Percent variance',
col=`\#BBDDFF') dev.copy2pdf(file=``pca\_variances.pdf'', width=7,
height=5) ```

\subsubsection{Analysis of the first versus second
component}\label{analysis-of-the-first-versus-second-component}

We can generate a {biplot}, where each sample appears as a dot, and the
X and Y axes respectively represent the first and second components of
the PCA-transformed data. The R function \emph{stats::biplot()}
automatically generates the biplot, but the result is somewhat
confusing, because the biplot displays the whole sample labels.

\texttt{\{r fig.width=8, fig.height=8\} biplot(expr.prcomp,var.axes=FALSE,        panel.first=grid(col='black'),         main=paste('PCA; Den Boer (2009); ',        ncol(expr.matrix), 'samples *',         nrow(expr.matrix), 'genes', sep=' '),         xlab='First component', ylab='Second component')}

Let us now generate a custom plot, with labels and colors indicating the
subtype of ALL.

\texttt{\{r fig.width=8, fig.height=8\} \#\# Plot components PC1 and PC2 plot(expr.prcomp\$x{[},1:2{]},        col=sample.colors,        type='n',        panel.first=c(grid(col='black'), abline(a=30,b=1.3, lwd=3, lty="dashed", col="red")),          main=paste('PCA; Den Boer (2009); ',            ncol(expr.matrix), 'samples *', nrow(expr.matrix), 'genes', sep=' '),           xlab='PC1', ylab='PC2') text(expr.prcomp\$x{[},1:2{]},labels=sample.labels,col=sample.colors,pch=0.5) legend('bottomleft',col=group.colors,           legend=names(group.colors),pch=1,cex=0.6,bg='white',bty='o') dev.copy2pdf(file="PC1\_vs\_PC2.pdf", width=8, height=8)}

The coloring and cancer-type labels highlight a very interesting
property of the PCA result: in the plane defined by the two first
components of the PCA-transformed data, we can draw a straight line that
perfectly separates T-ALL samples from all other subtypes (red dashed
line on the plot). All T-ALL cells are grouped in one elongated cloud on
the upper left side of the plot. The other cloud seems to contain some
organization as well: subtypes are partly intermingled but there are
obvious groupings.

\subsubsection{Analysis of the second versus third
component}\label{analysis-of-the-second-versus-third-component}

The following figure shows that the second and third components capture
information related to the cancer subtypes: the third component
separates quite well TEL-AML1 (top) from hyperdiploid (bottom) samples,
whereas the pre-B have intermediate values. The separation is however
less obvious than the T-ALL versus all other subtypes that we observed
in the two first components.

\texttt{\{r fig.width=8, fig.height=8\} \#\# Plot components PC2 and PC3 plot(expr.prcomp\$x{[},2:3{]},        col=sample.colors,        type='n',        panel.first=grid(col='black'),           main=paste('PCA; Den Boer (2009); ',            ncol(expr.matrix), 'samples *', nrow(expr.matrix), 'genes', sep=' '),           xlab='PC2', ylab='PC3') text(expr.prcomp\$x{[},2:3{]},labels=sample.labels,col=sample.colors,pch=0.5)      legend('bottomleft',col=group.colors,           legend=names(group.colors),pch=1,cex=0.6,bg='white',bty='o') dev.copy2pdf(file="PC2\_vs\_PC3.pdf", width=8, height=8)}

\subsubsection{Exercise}\label{exercise}

Generate plots of a few additional components (PC4, PC5,\ldots{}) and
try to evaluate if they further separate subtypes of cancers.

\subsubsection{Discussion of the PCA
results}\label{discussion-of-the-pca-results}

In the ``historical'' dataset from Golub (1999), three sample groups
(AML, T-ALL and B-ALL) appeared almost perfectly separated by a simple
display of the two first components of the PCA-transformed data. The
dataset from DenBoer (2009) is less obvious to interpret, due to the
nature of the data itself: firstly, all the samples come from the same
cell type (lymphoblasts), whereas the main separation of Golub was
between myeloblasts and lymphoblasts. Secondly, Den Boer and co-workers
defined a wider variety of subtypes. However, we see that a simple PCA
transformation already reveals that the raw expression data is well
structured. It is quite obvious that we will have no difficulty to find
a signature discriminating T-ALL from other ALL subtypes, since T-ALL
are already separated on the plane of the two first components. We would
however like to further refine the diagnostics, by training a classifier
to discriminate between the other subtypes as well.

Note that until now we did not apply any training: PCA transformation is
a ``blind'' (unsupervised) approach, performing a rotation in the
variable space without using any information about pre-defined classes.
Since we dispose of such information for the 190 samples of the training
set, we can use a family of other approaches, generically called
{supervised classification}, that will rely on class membership of the
training samples in order to train a classifier, which will further be
used to assign new samples to the same classes.

\subsubsection{Selecting principal components as predictor
variables\}}\label{selecting-principal-components-as-predictor-variables}

Optionnally, principal components can be used as predictor variables for
supervised classification. The advantage is that the first components
supposedly concentrate more information than any individual variables,
and the classifier is thus in principle able to achieve a better
discrimination with less variables (and thus be less prone to
over-fitting). However, we should keep in mind that variables showing a
high variance may differ from the most discriminating ones.

\subsection{Linear Discriminant Analysis
(LDA)}\label{linear-discriminant-analysis-lda}

\subsubsection{Training the classifier}\label{training-the-classifier}

We will define a vector which will associate each individual (sample) to
either of two labels: Bo for pre-B ALL samples, and ``o'' for all the
other subtypes.

\texttt{\{r\} \#\# Define the labels for a 2-groups classifier group.of.interest \textless{}- "Bo" one.vs.others\textless{}- sample.labels one.vs.others{[}sample.labels != group.of.interest{]} \textless{}- "o" print(table(one.vs.others))}

We can now use our dataset to train a linear discriminant classifier,
using the R \emph{lda()} function. In a first time, we will use the
whole dataset to train the classifier.

Warning: the step below costs time, memory, and is expected to give very
bad results. We run it only for didactic purpose.

Indeed, the goal of this tutorial is to introduce the problems of
{overfitting} due to the {over-dimensionality} of the variable space,
and to show how this problem can be circumvented by selecting a relevant
subset of the variables before running the supervised classification.

Feature selection is particularly important when - as in the current
case - the dataset contains many more variables (\textgreater{}22.000
genes) than objects (190 samples). In the first trial below we will
intently ignore this over-dimensionality of the dataset, and measure the
consequences of this bad choice. In the following sections, we will show
how to resolve the problem of over-dimensionality by selecting subsets
of variables.

```\{r\} \#\# Load the MASS library, which contains the lda() function
library(MASS)

\subsection{Train a classifier}\label{train-a-classifier}

one.vs.others.lda.allvars \textless{}-
lda(t(expr.matrix),one.vs.others,CV=FALSE) ```

The \emph{lda()} function issues a warning, indicating that there is a
collinearity between some variables. Let us ignore this warning for the
time being (this problem will be solved below by selecting datasets with
less variables).

Let us inspect the result of lda().
\texttt{\{r\} attributes(one.vs.others.lda.allvars)}

Let us inspect the contents of the lda result.

\texttt{\{r\} print(one.vs.others.lda.allvars\$counts) print(one.vs.others.lda.allvars\$prior) print(one.vs.others.lda.allvars\$svd)}

\begin{itemize}
\item
  \textbf{counts} indicates the number of samples belonging to the
  different training classes (44 pre-ALL B with label ``Bo'', and 146
  others with label ``o'')
\item
  \textbf{prior} gives the {prior probabilities} of each class, i.e.~the
  probability for a sample to belong to either of the classes (in our
  case: 23\% Bo, 77\% others). By default, priors are simply estimated
  from the proportion of objects in the training classes, but an
  experienced user might decide to impose different priors based for
  example on additional information about the population.
\item
  \textbf{svd}, the singular value, indicates the ratio of the
  between-group and within-group standard deviations. We obtain a value
  of 11.17, suggesting that the classifier will be quite good at
  discriminating the Bo group from the other samples.
\end{itemize}

You can inspect yourself the other attributes (N, lev, call)

\subsubsection{Using the classifier to assign elements to
classes}\label{using-the-classifier-to-assign-elements-to-classes}

After having trained the classifier, we can use it to classify new
samples. A naive - but {wrong} - way to test the efficiency of our
classifier would be to use it to predict the class of our training
objects.

The R \emph{predict()} function is used to assign a class to a
\emph{new} dataset, based on an object generated by the \emph{lda()}
function. We use it here in a foolish way, to predict class of the
training set itself (we will show below the problem).

```\{r\} \#\# Non-recommended usage of the predict() function, since we
assign a class to the training objects themselves predict.lda.allvars
\textless{}- predict(object=one.vs.others.lda.allvars, newdata =
t(expr.matrix))

attributes(predict.lda.allvars)

\subsection{Posterior probabilities (print the 10 first rows
only)}\label{posterior-probabilities-print-the-10-first-rows-only}

print(predict.lda.allvars\$posterior{[}1:10,{]})

\subsection{Predicted classes}\label{predicted-classes}

print(predict.lda.allvars\$class) ```

The predictor computed the {posterior probability} of each object
(sample) to belong to the two training classes (``Bo''" or ``others''``,
respectively), and assignedeach object to one of these classes based on
the highest posterior.

We can now compare training and predicted classes, and measure the {hit
rate}, i.e.~the proportion of objects assigned to the correct class.

```\{r\} \#\# Print the contingency table table(one.vs.others,
predict.lda.allvars\$class)

\subsection{Compute the hit rate}\label{compute-the-hit-rate}

hits \textless{}- sum(one.vs.others ==
predict.lda.allvars$class) errors <- sum(one.vs.others != predict.lda.allvars$class)
total \textless{}- hits + errors (hit.rate.internal \textless{}- hits /
total) (error.rate.internal \textless{}- errors / total) ```

\textbf{Beware:} what we did above is absolutely forbidden: we used a
training set to train a classifier, and we assessed the precision of
this classifier based on the training set itself.

\subsection{Evaluating the classifier by
cross-validation}\label{evaluating-the-classifier-by-cross-validation}

\subsubsection{K-fold cross-validation}\label{k-fold-cross-validation}

A fair evaluation of a classifier requires to use separate datasets for
training and testing, respectively.

A simple way to achieve this is the {2-fold cross-validation} to
randomly split the original set in two equal parts, and use one for
training and the other one for testing. Since the left-out subset has
not been used for training, the evaluation is indicative of the expected
accuracy when the classifier will be used to predict the class of new
samples. However, a classifier trained with only half of the samples
will be suboptimal. Splitting the dataset in two equal parts would thus
lead to a sub-esimation of its predictive power.

A more refined approach, called {k-fold cross-validation}, is to
randomly split the data set in $k$ subsets. We can discard one subset,
train a classifier with the $k-1$ remaining subsets, and predict the
class of the $k^{th}$ subset.

\subsubsection{Leave-One-Out (LOO)
cross-validation}\label{leave-one-out-loo-cross-validation}

The {Leave-one-out} ({LOO}) is a particular mode of cross-validation,
where each element of the training set will be in turn discarded (left
out) to evaluate a classifier trained with all the other elements. This
is the most economical way to perform a k-fold cross-validation.
Actually, LOO can be understood as ``n-fold'' cross-validation, if $n$
is the number of samples of the training set.

The R function \emph{lda()} includes a very convenient option
(\emph{cv}, for \emph{cross-validation}) that automatically runs LOO
cross-validation.

```\{r\} \#\# Run a Leave-one-out (LOO) cross-validation
one.vs.others.lda.allvars.loo \textless{}-
lda(t(expr.matrix),one.vs.others,CV=TRUE)

table(one.vs.others, one.vs.others.lda.allvars.loo\$class)

\subsection{Compute the hit rate}\label{compute-the-hit-rate-1}

(hit.rate.loo \textless{}- sum(one.vs.others ==
one.vs.others.lda.allvars.loo\$class) / total) (error.rate.loo
\textless{}- 1 - hit.rate.loo) ```

We have an obvious contradiction:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The (biased) internal validation shows that, after training, the
  classifier is able to re-assign 88\% of the samples to the right
  class, among pre-B ALL and the other subtypes.
\item
  An unbiased validation based on the Leave-one-out test shows that the
  apparent accuracy is a trap: when the classifier has to assign a class
  to a sample not seen before (because it was left out during the
  training), the hit rate drops to 53\%.
\end{enumerate}

\subsubsection{Random expectation for the hit
rate}\label{random-expectation-for-the-hit-rate}

We could push further our investigation, by estimating the random
expectation for the hit rate. We can test this with a very simple test:
let us imagine a random classifier, which would assign labels ``Bo'' or
``o'' to each sample according to their prior probabilities (23\% and
77\% resp.).

A very simple way to simulate such a random classifier is to permute the
vector of training labels, and to compute the hit rate between permuted
and original labels. We can repeat this random sampling $10.000$ times
and plot an histogram, which will indicate not only the mean behaviour
but also the fluctuation around it.

We can also compute the theoretical expectation for the random hit rate.

```\{r\} \#\# Run 10000 permutation tests to estimate the random
expectation for the hit rate random.hit.rates \textless{}- vector() for
(rep in 1:10000) \{ random.hit.rates \textless{}-
append(random.hit.rates, sum(one.vs.others == sample(one.vs.others)) /
total) \} (random.hit.rates.mean \textless{}- mean(random.hit.rates))

\subsection{Compute the theoretical value for the random
expectation}\label{compute-the-theoretical-value-for-the-random-expectation}

prior \textless{}- as.vector(table(one.vs.others))/length(one.vs.others)
(hit.rate.expect \textless{}- sum(prior\^{}2))

\subsection{Draw the histogram}\label{draw-the-histogram}

hist(random.hit.rates, breaks=(0:total)/total, col=``lightgrey'',
freq=TRUE, main=``Hit rate analysis'', xlab=``Hit rate'',
ylab=``Frequency'') arrows(x0=hit.rate.loo, y0 = 1000, x1=hit.rate.loo,
y1=100, col=``darkgreen'', lwd=2, code=2, , length=0.2, angle=20)
arrows(x0=hit.rate.internal, y0 = 1000, x1=hit.rate.internal, y1=100,
col=``red'', lwd=2, code=2, , length=0.2, angle=20)
arrows(x0=hit.rate.expect, y0 = 1000, x1=hit.rate.expect, y1=100,
col=``darkblue'', lwd=2, code=2, , length=0.2, angle=20)

legend(``topleft'', legend=c(``random'', ``random expectation'',
``LOO'', ``internal''), col=c(``grey'', ``darkblue'', ``darkgreen'',
``red''), lwd=c(4,2,2,2))

```

\subsubsection{The problem of
over-fitting}\label{the-problem-of-over-fitting}

\subsubsection{Selecting a subset of the variables (feature
selection)}\label{selecting-a-subset-of-the-variables-feature-selection}

In a first attempt to discriminate genes from the Bo class (pre-B-ALL),
we will select top-ranking genes according to their significance of the
Welch test (Bo versus others).

```\{r\}

\subsection{Test the mean equality between Bo subtype and all other
subtypes}\label{test-the-mean-equality-between-bo-subtype-and-all-other-subtypes}

welch.Bo.vs.others \textless{}- t.test.multi(expr.matrix, one.vs.others,
volcano.plot = FALSE)

\subsection{Print the names of the 20 top-ranking
genes.}\label{print-the-names-of-the-20-top-ranking-genes.}

\subsection{Note that these are not sorted
!}\label{note-that-these-are-not-sorted}

print(rownames(welch.Bo.vs.others{[}welch.Bo.vs.others\$rank
\textless{}= 20,{]}))

\subsection{Sort gene names by Welch
sig}\label{sort-gene-names-by-welch-sig}

welch.Bo.vs.others.sorted \textless{}-
welch.Bo.vs.others{[}order(welch.Bo.vs.others\$sig, decreasing=TRUE),{]}
sorted.names \textless{}- rownames(welch.Bo.vs.others.sorted)
print(sorted.names{[}1:20{]})
welch.Bo.vs.others{[}sorted.names{[}0:20{]}, c(``E.value'',``sig''){]}
```

For the sake of curiosity, we can draw a plot with the two top-ranking
genes in order to see how well they separate our group of interest
(pre-B all) from all the other ALL subtypes.

``` \{r fig.width=8, fig.height=8\} \#\# Print a plot with the two
top-ranking genes with the Bo versus other Welch test g1 \textless{}-
sorted.names{[}1{]} g2 \textless{}- sorted.names{[}2{]} x \textless{}-
as.vector(as.matrix(expr.matrix{[}g1,{]})) y \textless{}-
as.vector(as.matrix(expr.matrix{[}g2,{]})) plot(x,y, col=sample.colors,
type=``n'', panel.first=grid(col=``black''), main=``Den Boer (2009), 2
top-ranking genes for Welch test'', xlab=paste(``gene'', g1),
ylab=paste(``gene'', g2)) text(x,
y,labels=one.vs.others,col=sample.colors,pch=0.5)
legend(``bottomright'',col=group.colors,
legend=names(group.colors),pch=1,cex=0.6,bg=``white'',bty=``o'')

dev.copy2pdf(file=paste(sep=``'', group.of.interest,
``\emph{vs}others\_Welch\_g1\_g2.pdf''), width=8, height=8) ```

The plot shows a rather good general trends: Bo samples are concentrated
in the top-left side of the plot. They have thus generally a low level
of expression for the first gene (GPRASP1) and a high level for the
second one (CD19).

However, none of the two top-ranking genes is sufficiently discriminant
by its own: a selection based on GPRASP1 alone (a vertical boundary on
the plot) would fail to distinguish Bo genes from the other ones. The
same would be true for a selection based on CD19 alone (horizontal
boundary). We can push this experiment further by plotting the third and
fourth top-ranking genes according to Welch test.

``` \{r fig.width=8, fig.height=8\} \#\# Print a plot with the third and
fourth top-ranking genes with the Bo versus other Welch test g3
\textless{}- sorted.names{[}3{]} g4 \textless{}- sorted.names{[}4{]} x
\textless{}- as.vector(as.matrix(expr.matrix{[}g3,{]})) y \textless{}-
as.vector(as.matrix(expr.matrix{[}g4,{]}))

plot(x,y, col=sample.colors, type=``n'',
panel.first=grid(col=``black''), main=``Den Boer (2009), top-ranking
genes 3 and 4 for Welch test'', xlab=paste(``gene'', g3),
ylab=paste(``gene'', g4)) text(x,
y,labels=one.vs.others,col=sample.colors,pch=0.5)
legend(``bottomright'',col=group.colors,
legend=names(group.colors),pch=1,cex=0.6,bg=``white'',bty=``o'')
dev.copy2pdf(file=paste(sep=``'', group.of.interest,
``\emph{vs}others\_Welch\_g3\_g4.pdf''), width=8, height=8) ```

Here again, we would not be able to draw a boundary that would perfectly
separate the samples of our group of interest from all other ones.

The principle of discriminant analysis is to establish a much more
refined criterion to discriminate objects (samples in our cases), by
computing a score as a linear combination of the variables. A priori, we
could feed the linear discriminant classifier (the R \emph{lda()}
function) with all the variables of the data table (all the genes).
However, this would probably return poor results, because when the
number of features (genes) is larger than the number of individuals
(samples), the classifier will tend to be overfit to the circumstantial
particularities of the training objects, and it will fail to classify
new objects.

It is thus very important to select a relevant number of variables
before training a classifier. It is almost an art to select the most
appropriate selection criterion and the optimal number of variables. In
this course we will only present some basic methods.

```\{r\} \#\# Load the library containing the linear discriminant
analysis function library(MASS)

\subsection{Select the 20 top-ranking genes sorted by significance of
the Welch
test}\label{select-the-20-top-ranking-genes-sorted-by-significance-of-the-welch-test}

top.variables \textless{}- 20 selected.genes \textless{}-
sorted.names{[}1:top.variables{]}

\subsection{Train the classifier}\label{train-the-classifier}

one.vs.others.lda.classifier \textless{}-
lda(t(expr.matrix{[}selected.genes,{]}),one.vs.others,CV=FALSE) ```

\subsubsection{Evaluating the hit rate by Leave-One-Out (LOO)
cross-validation\}}\label{evaluating-the-hit-rate-by-leave-one-out-loo-cross-validation}

The \emph{lda()} function includes an option to automatically run a
cross-validation test (\emph{cv}), which relies on the {Leave-one-out}
procedure ({LOO}).

The principle of this test is to temporarily discard one element from
the training set ({left out} element), to train a classifier with the
remaining elements, and to use the trained classified to predict the
group of the left out element. The procedure is applied to each element
in turn. This cross-validation ensures a non-biased evaluation of the
rates of correct and incorrect classification.

``` \{r\} \#\# Use the MASS:lda() function with the cross-validation
option one.vs.others.lda.loo \textless{}-
lda(t(expr.matrix{[}selected.genes,{]}),one.vs.others,CV=TRUE)

\subsection{Collect the LOO prediction result in a
vector}\label{collect-the-loo-prediction-result-in-a-vector}

one.vs.others.loo.predicted.class \textless{}-
as.vector(one.vs.others.lda.loo\$class)
table(one.vs.others.loo.predicted.class)

\subsection{Build a confusion table of known versus predicted
class}\label{build-a-confusion-table-of-known-versus-predicted-class}

(one.vs.others.lda.loo.xtab \textless{}- table(one.vs.others,
one.vs.others.loo.predicted.class))

\subsection{Build a more general contingency table to check the
assignation}\label{build-a-more-general-contingency-table-to-check-the-assignation}

\subsection{of each known group to ``Bo'' (pre-B ALL) or ``o''
(other).}\label{of-each-known-group-to-bo-pre-b-all-or-o-other.}

\subsection{This permits to perform a more detailed analysis of
the}\label{this-permits-to-perform-a-more-detailed-analysis-of-the}

\subsection{misclassifications.}\label{misclassifications.}

(one.vs.others.lda.loo.xtab2 \textless{}- table(sample.labels,
one.vs.others.loo.predicted.class))

\subsection{Display the contingency table as a heat
map}\label{display-the-contingency-table-as-a-heat-map}

library(lattice) levelplot(one.vs.others.lda.loo.xtab)

\subsection{Compute the hit rate}\label{compute-the-hit-rate-2}

hits \textless{}- one.vs.others == one.vs.others.loo.predicted.class
errors \textless{}- one.vs.others != one.vs.others.loo.predicted.class

\subsection{Compute the number of
hits}\label{compute-the-number-of-hits}

\subsection{(we need to omit NA values because LDA fails to assign a
group to some
objects).}\label{we-need-to-omit-na-values-because-lda-fails-to-assign-a-group-to-some-objects.}

(nb.hits \textless{}- sum(na.omit(hits))) (nb.pred \textless{}-
length(na.omit(hits))) (hit.rate \textless{}- nb.hits / nb.pred ) ```

\subsection{Multi-group
classification}\label{multi-group-classification}

Linear discriminant analysis also enables to direcly classify a set of
individuals into multiple groups. We could for example use the original
sample labels to train a classifier, which would learn to recognize the
different subtypes of ALL.

``` \{r\} \#\# Use the MASS:lda() function with the cross-validation
option multigroup.lda.loo \textless{}-
lda(t(expr.matrix{[}selected.genes,{]}),sample.labels,CV=TRUE)

\subsection{Collect the LOO prediction result in a
vector}\label{collect-the-loo-prediction-result-in-a-vector-1}

multigroup.loo.predicted.class \textless{}-
as.vector(multigroup.lda.loo\$class)
table(multigroup.loo.predicted.class)

\subsection{Build a confusion table of known versus predicted
class}\label{build-a-confusion-table-of-known-versus-predicted-class-1}

(multigroup.lda.loo.xtab \textless{}- table(sample.labels,
multigroup.loo.predicted.class))

\subsection{Display the contingency table as a heat
map}\label{display-the-contingency-table-as-a-heat-map-1}

library(lattice) levelplot(multigroup.lda.loo.xtab)

\subsection{Compute the hit rate}\label{compute-the-hit-rate-3}

hits \textless{}- sample.labels == multigroup.loo.predicted.class errors
\textless{}- sample.labels != multigroup.loo.predicted.class

\subsection{Compute the number of
hits}\label{compute-the-number-of-hits-1}

\subsection{(we need to omit NA values because LDA fails to assign a
group to some
objects).}\label{we-need-to-omit-na-values-because-lda-fails-to-assign-a-group-to-some-objects.-1}

(nb.hits \textless{}- sum(na.omit(hits))) (nb.pred \textless{}-
length(na.omit(hits))) (hit.rate \textless{}- nb.hits / nb.pred ) ```

\subsubsection{Random expectation for the hit
rate}\label{random-expectation-for-the-hit-rate-1}

We could compute the random expectation of the hit rate. Let us imagine
a tricky classifier that woudl assign the labels randomly, for two
groups of equal sizes. Each object would have equal chances to be
assigned to it correct class, and the random assignation would thus
gvive 50\% of correct classification.

\paragraph{Random expectation for a 2-groups permutation of training
labels}\label{random-expectation-for-a-2-groups-permutation-of-training-labels}

However, in our case the training classes are unbalanced: $n_{GOI}=44$
samples belong to the group of interest, and $n_{others}$ to the other
groups. If samples are re-assigned randomly with proportions equal to
their prior frequencies, the assignation probabilities are:

\[p_{GOI}= n_{GOI}/n = 44/190=0.23\]

\[p_{other}= n_{GOI}/n = 146/190=0.77\]

A random assignation would thus assign correctly $23\%$ of pre-ALL
samples, and $77\%$ of the samples of other subtypes would be correctly
assigned to the group ``others''. We can thus compute the random
expectation for the hit rate.

\[\hat{hits}_{GOI} = n_{GOI} \cdot p_{GOI} + n_{others} \cdot p_{others} = 44 \cdot 0.232 + 146 \cdot 0.768 = 122.54\]

\[\had{hit rate}_{GOI} = \hat{hits}_{GOI} / n = 122.54/190 = 0.64 = 64%\]

Note the same result is obtained directly by taking the sum of squares
of prior probabilities:

\[\hat{hits}_{GOI} = p_{GOI}^2 + p_{others}^2 = 0.232^2 + 0.768^2 = 0.64 = 64%\]

```\{r\} n \textless{}- length(sample.labels) n.goi \textless{}-
sum(sample.labels == group.of.interest) n.others \textless{}-
sum(sample.labels != group.of.interest) prior \textless{}-
c(``goi''=n.goi/n, ``others'' = n.others/n) print(prior)

exp.hits \textless{}- c(``goi''=(n.goi\emph{prior{[}``goi''{]}),
``other''=(n.others}prior{[}``others''{]})) print(exp.hits)

exp.hit.rate \textless{}- sum(prior\^{}2) print(exp.hit.rate) ```

A random classifier which would respect the balance of the training
groups (and thus assign labels according to the group prior
probabilities) would thus already achieve a hit rate of $65\%$.

\paragraph{Random expectation for a multi-groups permutation of training
labels}\label{random-expectation-for-a-multi-groups-permutation-of-training-labels}

We can extend the above idea to estiamte the expected proportion of
correspondances between permuted labels in a multi-group configuration.

\texttt{\{r\} multi.samples.per.class \textless{}- unlist(table(sample.labels)) multi.prior \textless{}- (multi.samples.per.class)/sum(multi.samples.per.class) (multi.expect.hit.rate \textless{}- sum(multi.prior\^{}2))}

The random hit rate for permuted labels is the sum of square priors.

\[\hat{hit rate} = \sum_{i=1}^{g}{p_i}\]

Where $g$ is the number of groups (ALL subtypes) and $p_i$ the prior
probability of the $i- {th}$ group.

\subsection{Exercise: training a classifier with permuted
labels}\label{exercise-training-a-classifier-with-permuted-labels}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use the {R} function sample() to randomly permute the training labels
  (store the result in a vector called ``sample.labels.perm''), and test
  the hit rate of a classifier trained with these randomized labels.
\item
  Use the {R} function sample() to randomly permute the values of the
  input table (store the result in a data frame called ``expr.perm''),
  and test the hit rate of a classifier trained with these randomized
  data.
\end{enumerate}

\subsubsection{Solutions}\label{solutions}

We will successively run the permutation test with two configurations:
1. multi-group classification (using the original sample labels) 2.
two-group classification (pre-B all versus others)

\subsubsection{Label permutation test for multi-group
classification}\label{label-permutation-test-for-multi-group-classification}

``` \{r\} \#\# Permute the training labels sample.labels.perm
\textless{}- as.vector(sample(sample.labels))

\subsection{Compare original training groups and permuted
labels.}\label{compare-original-training-groups-and-permuted-labels.}

table(sample.labels, sample.labels.perm) ```

We can now run the leave-one-out (LOO) cross-validation with the
permuted labels.

``` \{r\} \#\# Run LDA in cross-validation (LOO) mode with the permuted
labels lda.loo.labels.perm \textless{}-
lda(t(expr.matrix{[}selected.genes,{]}),sample.labels.perm,CV=TRUE)

\subsection{Build a contingency table of known versus predicted
class}\label{build-a-contingency-table-of-known-versus-predicted-class}

loo.predicted.class.labels.perm \textless{}-
as.vector(lda.loo.labels.perm\$class) lda.loo.labels.perm.xtab
\textless{}- table(sample.labels.perm, loo.predicted.class.labels.perm)
print(lda.loo.labels.perm.xtab) ```

The particular values sould vary at each trial, since the sample labels
are permuted at random.

\texttt{\{r\} \#\# Compute the number of hits  \#\# (we need to omit NA values because LDA fails to assign a group to some objects). hits.label.perm \textless{}- sample.labels.perm == loo.predicted.class.labels.perm (nb.hits.label.perm \textless{}- sum(na.omit(hits.label.perm))) \#\# This gives 25 this time, but should give different results at each trial (nb.pred.label.perm \textless{}- length(na.omit(hits.label.perm))) \#\# This should give 187 (hit.rate.label.perm \textless{}- nb.hits.label.perm / nb.pred.label.perm ) \#\# This gives 0.13 this time, should give different results at each trial}

It might seem surprizing to obtain an apparently decent hit rate
(\textasciitilde{}75\%) with the permutation test. Indeed, since sample
labels were assigned at random, the classifer sould not be able to learn
anything.

\subsubsection{Label permutation test for two-group
classification}\label{label-permutation-test-for-two-group-classification}

``` \{r\} \#\# Permute the training labels sample.labels.perm.2gr
\textless{}- as.vector(sample(one.vs.others))

\subsection{Compare original training groups and permuted
labels.}\label{compare-original-training-groups-and-permuted-labels.-1}

table(one.vs.others, sample.labels.perm.2gr)

\subsection{Compute the percent of correspondence between training
groups}\label{compute-the-percent-of-correspondence-between-training-groups}

\subsection{and permuted labels}\label{and-permuted-labels}

permuted.equal \textless{}- sum(diag(table(one.vs.others,
sample.labels.perm.2gr))) (permuted.equal.rate \textless{}-
permuted.equal/length(one.vs.others)) ```

After permutation, each of the permuted classes should contain on
average \textasciitilde{}23\% of samples from the ``Bo'' subtype, and
\textasciitilde{}77\% of sample from other subtypes (the precise numbers
fary between repetitions of the random sampling).

Note that in the two-groups configuration we already have 64\% of
equality between the original and permuted labels. This rate is higher
than 50\% because the groups are unbalanced: a label ``other'' will
frequently be permuted with another label ``other''.

We can now run the leave-one-out (LOO) cross-validation with the
permuted labels.

``` \{r\} \#\# Run LDA in cross-validation (LOO) mode with the permuted
labels lda.loo.labels.perm.2gr \textless{}-
lda(t(expr.matrix{[}selected.genes,{]}),sample.labels.perm.2gr,CV=TRUE)

\subsection{Build a contingency table of known versus predicted
class}\label{build-a-contingency-table-of-known-versus-predicted-class-1}

loo.predicted.class.labels.perm.2gr \textless{}-
as.vector(lda.loo.labels.perm.2gr\$class) lda.loo.labels.perm.2gr.xtab
\textless{}- table(sample.labels.perm.2gr,
loo.predicted.class.labels.perm.2gr) print(lda.loo.labels.perm.2gr.xtab)

\subsection{Compute the hit rate of the classifier trained with permuted
labels}\label{compute-the-hit-rate-of-the-classifier-trained-with-permuted-labels}

(hit.rate.label.perm.2gr \textless{}-
sum(diag(lda.loo.labels.perm.2gr.xtab)) /
sum(lda.loo.labels.perm.2gr.xtab)) ```

It might seem surprizing to obtain an apparently good hit rate
(\textasciitilde{}75\%) with the permutation test. The \emph{lda()}
trained with permuted labels achieved a better classification than the
expected hit for a random assignation. Does it mean that the program was
able to learn anything from randomly permuted labels?

Actually, the permuted samples still contain a informative indication:
the classes are unbalanced, with 77\% of the samples belonging to the
group labelled ``other''. A ``gambler'' classifier that would assign all
elements to the majority class would always achieve a better hit rate
than a random classifier. On average, this ``gambler'' classifier would
achieve a 77\% hit rate in an unbalanced two-groups configuration like
ours.

The \emph{lda()} classifier cannot be properly speaking qualified of
gambler. However, the confusion table reveals that, indeed, during the
LOO test, the \emph{lda()} classified tends to assign many samples to
the ``other'' (more than their prior frequency), which explains its
relatively high hit rate of \textasciitilde{}75\% in this permutation
test.

In summary, we should always be \textbf{very cautious} when interpreting
the results of supervised classification. Indeed, an apparently good hit
rate might be achieved even in absence of any informative difference
between the groups, by simple effects of the group sizes. This effect is
particularly important for a two-group classification, but can also
appear in multi-group classification with strongly unbalanced groups.

The reliability of a classifier should thus be estimated by comparing
its performances on a real dataset with the results obtained in
permutation tests.

\subsection{Exercises}\label{exercises}

\subsubsection{Exercise: Impact of the number of training
variables}\label{exercise-impact-of-the-number-of-training-variables}

Test the impact of the number of variables used for training the
classifier. Draw a plot representing the hit rate (Y axis) as a function
of the number of top genes used for the LDA classification, using
different selection criteria:

\begin{verbatim}
* Gene-wise variance
  * P-value of a Welch test (T-cells against all other types)
* ANOVA
\end{verbatim}

\subsubsection{Random expectation}\label{random-expectation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Generate a randomized expression matrix by sampling the values of the
  Den Boer dataset with the R function sample() (as we did in the
  practical on multiple testing corrections).
\item
  Run the same procedure with this randomized matrix as we did above
  with the original expression matrix, and calculate hit rate with
  increasing number of variables selected by
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Variance
\item
  Welch test (T-cells against all other types)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Compare the hit rates obtained with the permuted matrix and with the
  original expression matrix.
\end{enumerate}

\subsection{Other classification
approaches}\label{other-classification-approaches}

\subsubsection{Quadratic discrminant analysis
(QDA)}\label{quadratic-discrminant-analysis-qda}

```\{r\} (n.variables \textless{}- length(selected.genes))

\subsection{train a quadratic disctiminant analysis (QDA)
classifier}\label{train-a-quadratic-disctiminant-analysis-qda-classifier}

one.vs.others.qda.loo \textless{}-
qda(t(expr.matrix{[}selected.genes,{]}),one.vs.others,CV=TRUE)
table(one.vs.others,
one.vs.others.qda.loo$class) (one.vs.others.qda.loo.hit.rate <- sum(one.vs.others == one.vs.others.qda.loo$class)/n)

\subsection{Permutation test on labels with quadratic disctiminant
analysis (QDA)
classifier}\label{permutation-test-on-labels-with-quadratic-disctiminant-analysis-qda-classifier}

label.perm.one.vs.others.qda.loo \textless{}-
qda(t(expr.matrix{[}selected.genes,{]}), sample(one.vs.others,
replace=FALSE), CV=TRUE) table(one.vs.others,
label.perm.one.vs.others.qda.loo$class) (label.perm.one.vs.others.qda.loo.hit.rate <-     sum(one.vs.others == label.perm.one.vs.others.qda.loo$class)/n)

\subsection{Redo the same with LDA, for
comparison}\label{redo-the-same-with-lda-for-comparison}

one.vs.others.lda.loo \textless{}-
lda(t(expr.matrix{[}selected.genes,{]}),one.vs.others,CV=TRUE)
table(one.vs.others,
one.vs.others.lda.loo$class) (one.vs.others.lda.loo.hit.rate <- sum(one.vs.others == one.vs.others.lda.loo$class)/n)

```

\subsubsection{K-nearest-neighbours (KNN)
classifier\}}\label{k-nearest-neighbours-knn-classifier}

\subsubsection{Support vector machines\}}\label{support-vector-machines}

===========================================

\href{http://denis.puthier.perso.esil.univmed.fr/}{Denis Puthier} \&
\href{http://jacques.van-helden.perso.luminy.univmed.fr/}{Jacques van
Helden}
